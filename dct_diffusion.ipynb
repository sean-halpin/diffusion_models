{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dct_diffusion.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMHX++EGiA7eSi5hXn+cSeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/diffusion_models/blob/main/dct_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "ZGNTdeclPfZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | xargs -IXX pip uninstall -y XX"
      ],
      "metadata": {
        "id": "LD-CSTRo-LNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Theano"
      ],
      "metadata": {
        "id": "4WgkYQS15BKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "6b-Sl-sh5qRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install picklable-itertools==0.1.1"
      ],
      "metadata": {
        "id": "vhAD3zvt_5cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install progressbar2==3.10.0"
      ],
      "metadata": {
        "id": "TZTnZxWT_7o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==3.11"
      ],
      "metadata": {
        "id": "l2pVitJa_9o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install six==1.15.0"
      ],
      "metadata": {
        "id": "bfvzHmlK__7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toolz==0.9.0"
      ],
      "metadata": {
        "id": "uJOB87p8AB2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/Theano/Theano.git#egg=theano"
      ],
      "metadata": {
        "id": "3-C1DMO2AD6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/sean-halpin/fuel.git"
      ],
      "metadata": {
        "id": "Jg3BLmOPAFiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --verbose git+https://github.com/mila-iqia/blocks.git"
      ],
      "metadata": {
        "id": "Vd5TVX-hNtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOT_SWd6zDmD"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import theano\n",
        "import theano.tensor as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "from blocks.algorithms import (RMSProp, GradientDescent, CompositeRule,RemoveNotFinite)\n",
        "from blocks.extensions import FinishAfter, Timing, Printing\n",
        "from blocks.extensions.monitoring import (TrainingDataMonitoring,DataStreamMonitoring)\n",
        "from blocks.extensions.saveload import Checkpoint\n",
        "from blocks.extensions.training import SharedVariableModifier\n",
        "from blocks.filter import VariableFilter\n",
        "from blocks.graph import ComputationGraph, apply_dropout\n",
        "from blocks.main_loop import MainLoop\n",
        "import blocks.model\n",
        "from blocks.roles import INPUT, PARAMETER\n",
        "\n",
        "from fuel.streams import DataStream\n",
        "from fuel.schemes import ShuffledScheme\n",
        "from fuel.transformers import Flatten, ScaleAndShift\n"
      ],
      "metadata": {
        "id": "kHlutuUD2Me3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers"
      ],
      "metadata": {
        "id": "J217920rPIBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Viz"
      ],
      "metadata": {
        "id": "re1fRezTQBkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tools for plotting / visualization\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # no displayed figures -- need to call before loading pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "def is_square(shp, n_colors=1):\n",
        "    \"\"\"\n",
        "    Test whether entries in shp are square numbers, or are square numbers after divigind out the\n",
        "    number of color channels.\n",
        "    \"\"\"\n",
        "    is_sqr = (shp == np.round(np.sqrt(shp))**2)\n",
        "    is_sqr_colors = (shp == n_colors*np.round(np.sqrt(np.array(shp)/float(n_colors)))**2)\n",
        "    return is_sqr | is_sqr_colors\n",
        "\n",
        "def show_receptive_fields(theta, P=None, n_colors=None, max_display=100, grid_wa=None):\n",
        "    \"\"\"\n",
        "    Display receptive fields in a grid. Tries to intelligently guess whether to treat the rows,\n",
        "    the columns, or the last two axes together as containing the receptive fields. It does this\n",
        "    by checking which axes are square numbers -- so you can get some unexpected plots if the wrong\n",
        "    axis is a square number, or if multiple axes are. It also tries to handle the last axis\n",
        "    containing color channels correctly.\n",
        "    \"\"\"\n",
        "\n",
        "    shp = np.array(theta.shape)\n",
        "    if n_colors is None:\n",
        "        n_colors = 1\n",
        "        if shp[-1] == 3:\n",
        "            n_colors = 3\n",
        "    # multiply colors in as appropriate\n",
        "    if shp[-1] == n_colors:\n",
        "        shp[-2] *= n_colors\n",
        "        theta = theta.reshape(shp[:-1])\n",
        "        shp = np.array(theta.shape)\n",
        "    if len(shp) > 2:\n",
        "        # merge last two axes\n",
        "        shp[-2] *= shp[-1]\n",
        "        theta = theta.reshape(shp[:-1])\n",
        "        shp = np.array(theta.shape)\n",
        "    if len(shp) > 2:\n",
        "        # merge leading axes\n",
        "        theta = theta.reshape((-1,shp[-1]))\n",
        "        shp = np.array(theta.shape)\n",
        "    if len(shp) == 1:\n",
        "        theta = theta.reshape((-1,1))\n",
        "        shp = np.array(theta.shape)\n",
        "\n",
        "    # figure out the right orientation, by looking for the axis with a square\n",
        "    # number of entries, up to number of colors. transpose if required\n",
        "    is_sqr = is_square(shp, n_colors=n_colors)\n",
        "    if is_sqr[0] and is_sqr[1]:\n",
        "        warnings.warn(\"Unsure of correct matrix orientation. \"\n",
        "            \"Assuming receptive fields along first dimension.\")\n",
        "    elif is_sqr[1]:\n",
        "        theta = theta.T\n",
        "    elif not is_sqr[0] and not is_sqr[1]:\n",
        "        # neither direction corresponds well to an image\n",
        "        # NOTE if you delete this next line, the code will work. The rfs just won't look very\n",
        "        # image like\n",
        "        return False\n",
        "\n",
        "    theta = theta[:,:max_display].copy()\n",
        "\n",
        "    if P is None:\n",
        "        img_w = int(np.ceil(np.sqrt(theta.shape[0]/float(n_colors))))\n",
        "    else:\n",
        "        img_w = int(np.ceil(np.sqrt(P.shape[0]/float(n_colors))))\n",
        "    nf = theta.shape[1]\n",
        "    if grid_wa is None:\n",
        "        grid_wa = int(np.ceil(np.sqrt(float(nf))))\n",
        "    grid_wb = int(np.ceil(nf / float(grid_wa)))\n",
        "\n",
        "    if P is not None:\n",
        "        theta = np.dot(P, theta)\n",
        "\n",
        "    vmin = np.min(theta)\n",
        "    vmax = np.max(theta)\n",
        "\n",
        "    for jj in range(nf):\n",
        "        plt.subplot(grid_wa, grid_wb, jj+1)\n",
        "        ptch = np.zeros((n_colors*img_w**2,))\n",
        "        ptch[:theta.shape[0]] = theta[:,jj]\n",
        "        if n_colors==3:\n",
        "            ptch = ptch.reshape((n_colors, img_w, img_w))\n",
        "            ptch = ptch.transpose((1,2,0)) # move color channels to end\n",
        "        else:\n",
        "            ptch = ptch.reshape((img_w, img_w))\n",
        "        ptch -= vmin\n",
        "        ptch /= vmax-vmin\n",
        "        plt.imshow(ptch, interpolation='nearest', cmap=cm.Greys_r )\n",
        "        plt.axis('off')\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def plot_parameter(theta_in, base_fname_part1, base_fname_part2=\"\", title = '', n_colors=None):\n",
        "    \"\"\"\n",
        "    Save both a raw and receptive field style plot of the contents of theta_in.\n",
        "    base_fname_part1 provides the mandatory root of the filename.\n",
        "    \"\"\"\n",
        "\n",
        "    theta = np.array(theta_in.copy()) # in case it was a scalar\n",
        "    print(\"%s min %g median %g mean %g max %g shape\"%(\n",
        "        title, np.min(theta), np.median(theta), np.mean(theta), np.max(theta)), theta.shape)\n",
        "    theta = np.squeeze(theta)\n",
        "    if len(theta.shape) == 0:\n",
        "        # it's a scalar -- make it a 1d array\n",
        "        theta = np.array([theta])\n",
        "    shp = theta.shape\n",
        "    if len(shp) > 2:\n",
        "        theta = theta.reshape((theta.shape[0], -1))\n",
        "        shp = theta.shape\n",
        "\n",
        "    ## display basic figure\n",
        "    plt.figure(figsize=[8,8])\n",
        "    if len(shp) == 1:\n",
        "        plt.plot(theta, '.', alpha=0.5)\n",
        "    elif len(shp) == 2:\n",
        "        plt.imshow(theta, interpolation='nearest', aspect='auto', cmap=cm.Greys_r)\n",
        "        plt.colorbar()\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.savefig(base_fname_part1 + '_raw_' + base_fname_part2 + '.pdf')\n",
        "    plt.close()\n",
        "\n",
        "    ## also display it in basis function view if it's a matrix, or\n",
        "    ## if it's a bias with a square number of entries\n",
        "    if len(shp) >= 2 or is_square(shp[0]):\n",
        "        if len(shp) == 1:\n",
        "            theta = theta.reshape((-1,1))\n",
        "        plt.figure(figsize=[8,8])\n",
        "        if show_receptive_fields(theta, n_colors=n_colors):\n",
        "            plt.suptitle(title + \"receptive fields\")\n",
        "            plt.savefig(base_fname_part1 + '_rf_' + base_fname_part2 + '.pdf')\n",
        "        plt.close()\n",
        "\n",
        "def plot_images(X, fname):\n",
        "    \"\"\"\n",
        "    Plot images in a grid.\n",
        "    X is expected to be a 4d tensor of dimensions [# images]x[# colors]x[height]x[width]\n",
        "    \"\"\"\n",
        "    ## plot\n",
        "    # move color to end\n",
        "    Xcol = X.reshape((X.shape[0],-1,)).T\n",
        "    plt.figure(figsize=[8,8])\n",
        "    if show_receptive_fields(Xcol, n_colors=X.shape[1]):\n",
        "        plt.savefig(fname + '.pdf')\n",
        "    else:\n",
        "        warnings.warn('Images unexpected shape.')\n",
        "    plt.close()\n",
        "\n",
        "    ## save as a .npz file\n",
        "    np.savez(fname + '.npz', X=X)\n"
      ],
      "metadata": {
        "id": "lDgVgcmqPjWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampler"
      ],
      "metadata": {
        "id": "QD2o36s9QFPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# import viz\n",
        "\n",
        "def diffusion_step(Xmid, t, get_mu_sigma, denoise_sigma, mask, XT, rng):\n",
        "    \"\"\"\n",
        "    Run a single reverse diffusion step\n",
        "    \"\"\"\n",
        "    mu, sigma = get_mu_sigma(Xmid, np.array([[t]]))\n",
        "    if denoise_sigma is not None:\n",
        "        sigma_new = (sigma**-2 + denoise_sigma**-2)**-0.5\n",
        "        mu_new = mu * sigma_new**2 * sigma**-2 + XT * sigma_new**2 * denoise_sigma**-2\n",
        "        sigma = sigma_new\n",
        "        mu = mu_new\n",
        "    if mask is not None:\n",
        "        mu.flat[mask] = XT.flat[mask]\n",
        "        sigma.flat[mask] = 0.\n",
        "    Xmid = mu + sigma*rng.normal(size=Xmid.shape)\n",
        "    return Xmid\n",
        "\n",
        "\n",
        "def generate_inpaint_mask(n_samples, n_colors, spatial_width):\n",
        "    \"\"\"\n",
        "    The mask will be True where we keep the true image, and False where we're\n",
        "    inpainting.\n",
        "    \"\"\"\n",
        "    mask = np.zeros((n_samples, n_colors, spatial_width, spatial_width), dtype=bool)\n",
        "    # simple mask -- just mask out half the image\n",
        "    mask[:,:,:,spatial_width/2:] = True\n",
        "    return mask.ravel()\n",
        "\n",
        "\n",
        "def generate_samples(model, get_mu_sigma,\n",
        "            n_samples=36, inpaint=False, denoise_sigma=None, X_true=None,\n",
        "            base_fname_part1=\"samples\", base_fname_part2='',\n",
        "            num_intermediate_plots=4, seed=12345):\n",
        "    \"\"\"\n",
        "    Run the reverse diffusion process (generative model).\n",
        "    \"\"\"\n",
        "    # use the same noise in the samples every time, so they're easier to\n",
        "    # compare across learning\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    spatial_width = model.spatial_width\n",
        "    n_colors = model.n_colors\n",
        "\n",
        "    # set the initial state X^T of the reverse trajectory\n",
        "    XT = rng.normal(size=(n_samples,n_colors,spatial_width,spatial_width))\n",
        "    if denoise_sigma is not None:\n",
        "        XT = X_true + XT*denoise_sigma\n",
        "        base_fname_part1 += '_denoise%g'%denoise_sigma\n",
        "    if inpaint:\n",
        "        mask = generate_inpaint_mask(n_samples, n_colors, spatial_width)\n",
        "        XT.flat[mask] = X_true.flat[mask]\n",
        "        base_fname_part1 += '_inpaint'\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    if X_true is not None:\n",
        "        plot_images(X_true, base_fname_part1 + '_true' + base_fname_part2)\n",
        "    plot_images(XT, base_fname_part1 + '_t%04d'%model.trajectory_length + base_fname_part2)\n",
        "\n",
        "    Xmid = XT.copy()\n",
        "    for t in range(model.trajectory_length-1, 0, -1):\n",
        "        Xmid = diffusion_step(Xmid, t, get_mu_sigma, denoise_sigma, mask, XT, rng)\n",
        "        if np.mod(model.trajectory_length-t,\n",
        "            int(np.ceil(model.trajectory_length/(num_intermediate_plots+2.)))) == 0:\n",
        "            plot_images(Xmid, base_fname_part1 + '_t%04d'%t + base_fname_part2)\n",
        "\n",
        "    X0 = Xmid\n",
        "    plot_images(X0, base_fname_part1 + '_t%04d'%0 + base_fname_part2)\n"
      ],
      "metadata": {
        "id": "AdPPccwpQFnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Util"
      ],
      "metadata": {
        "id": "iNDoiAYsQxn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import time\n",
        "\n",
        "logit = lambda u: T.log(u / (1.-u))\n",
        "logit_np = lambda u: np.log(u / (1.-u)).astype(theano.config.floatX)\n",
        "\n",
        "def get_norms(model, gradients):\n",
        "    \"\"\"Compute norm of weights and their gradients divided by the number of elements\"\"\"\n",
        "    norms = []\n",
        "    grad_norms = []\n",
        "    for param_name, param in model.params.items():\n",
        "        norm = T.sqrt(T.sum(T.square(param))) / T.prod(param.shape.astype(theano.config.floatX))\n",
        "        norm.name = 'norm_' + param_name\n",
        "        norms.append(norm)\n",
        "        grad = gradients[param]\n",
        "        grad_norm = T.sqrt(T.sum(T.square(grad))) / T.prod(grad.shape.astype(theano.config.floatX))\n",
        "        grad_norm.name = 'grad_norm_' + param_name\n",
        "        grad_norms.append(grad_norm)\n",
        "    return norms, grad_norms\n",
        "\n",
        "def create_log_dir(args, model_id):\n",
        "    model_id += args.suffix + time.strftime('-%y%m%dT%H%M%S')\n",
        "    model_dir = os.path.join(os.path.expanduser(args.output_dir), model_id)\n",
        "    os.makedirs(model_dir)\n",
        "    return model_dir\n"
      ],
      "metadata": {
        "id": "UA9G6DQcQ0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extensions"
      ],
      "metadata": {
        "id": "LmZfUnm2P6Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extensions called during training to generate samples and diagnostic plots and printouts.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import theano\n",
        "\n",
        "from blocks.extensions import SimpleExtension\n",
        "\n",
        "# import viz\n",
        "# import sampler\n",
        "\n",
        "\n",
        "class PlotSamples(SimpleExtension):\n",
        "    def __init__(self, model, algorithm, X, path, n_samples=49, **kwargs):\n",
        "        \"\"\"\n",
        "        Generate samples from the model. The do() function is called as an extension during training.\n",
        "        Generates 3 types of samples:\n",
        "        - Sample from generative model\n",
        "        - Sample from image denoising posterior distribution (default signal to noise of 1)\n",
        "        - Sample from image inpainting posterior distribution (inpaint left half of image)\n",
        "        \"\"\"\n",
        "\n",
        "        super(PlotSamples, self).__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.path = path\n",
        "        n_samples = np.min([n_samples, X.shape[0]])\n",
        "        self.X = X[:n_samples].reshape(\n",
        "            (n_samples, model.n_colors, model.spatial_width, model.spatial_width))\n",
        "        self.n_samples = n_samples\n",
        "        X_noisy = T.tensor4('X noisy samp', dtype=theano.config.floatX)\n",
        "        t = T.matrix('t samp', dtype=theano.config.floatX)\n",
        "        self.get_mu_sigma = theano.function([X_noisy, t], model.get_mu_sigma(X_noisy, t),\n",
        "            allow_input_downcast=True)\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "\n",
        "        import sys\n",
        "        sys.setrecursionlimit(10000000)\n",
        "\n",
        "        print(\"generating samples\")\n",
        "        base_fname_part1 = self.path + '/samples-'\n",
        "        base_fname_part2 = '_batch%06d'%self.main_loop.status['iterations_done']\n",
        "        generate_samples(self.model, self.get_mu_sigma,\n",
        "            n_samples=self.n_samples, inpaint=False, denoise_sigma=None, X_true=None,\n",
        "            base_fname_part1=base_fname_part1, base_fname_part2=base_fname_part2)\n",
        "        generate_samples(self.model, self.get_mu_sigma,\n",
        "            n_samples=self.n_samples, inpaint=True, denoise_sigma=None, X_true=self.X,\n",
        "            base_fname_part1=base_fname_part1, base_fname_part2=base_fname_part2)\n",
        "        generate_samples(self.model, self.get_mu_sigma,\n",
        "            n_samples=self.n_samples, inpaint=False, denoise_sigma=1, X_true=self.X,\n",
        "            base_fname_part1=base_fname_part1, base_fname_part2=base_fname_part2)\n",
        "\n",
        "\n",
        "class PlotParameters(SimpleExtension):\n",
        "    def __init__(self, model, blocks_model, path, **kwargs):\n",
        "        super(PlotParameters, self).__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.model = model\n",
        "        self.blocks_model = blocks_model\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "\n",
        "        import sys\n",
        "        sys.setrecursionlimit(10000000)\n",
        "\n",
        "        print(\"plotting parameters\")\n",
        "        for param in self.blocks_model.parameters:\n",
        "            param_name = param.name\n",
        "            filename_safe_name = '-'.join(param_name.split('/')[2:]).replace(' ', '_')\n",
        "            base_fname_part1 = self.path + '/params-' + filename_safe_name\n",
        "            base_fname_part2 = '_batch%06d'%self.main_loop.status['iterations_done']\n",
        "            viz.plot_parameter(param.get_value(), base_fname_part1, base_fname_part2,\n",
        "                title=param_name, n_colors=self.model.n_colors)\n",
        "\n",
        "\n",
        "class PlotGradients(SimpleExtension):\n",
        "    def __init__(self, model, blocks_model, algorithm, X, path, **kwargs):\n",
        "        super(PlotGradients, self).__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.X = X\n",
        "        self.model = model\n",
        "        self.blocks_model = blocks_model\n",
        "        gradients = []\n",
        "        for param_name in sorted(self.blocks_model.parameters.keys()):\n",
        "            gradients.append(algorithm.gradients[self.blocks_model.parameters[param_name]])\n",
        "        self.grad_f = theano.function(algorithm.inputs, gradients, allow_input_downcast=True)\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "        print(\"plotting gradients\")\n",
        "        grad_vals = self.grad_f(self.X)\n",
        "        keynames = sorted(self.blocks_model.parameters.keys())\n",
        "        for ii in range(len(keynames)):\n",
        "            param_name = keynames[ii]\n",
        "            val = grad_vals[ii]\n",
        "            filename_safe_name = '-'.join(param_name.split('/')[2:]).replace(' ', '_')\n",
        "            base_fname_part1 = self.path + '/grads-' + filename_safe_name\n",
        "            base_fname_part2 = '_batch%06d'%self.main_loop.status['iterations_done']\n",
        "            viz.plot_parameter(val, base_fname_part1, base_fname_part2,\n",
        "                title=\"grad \" + param_name, n_colors=self.model.n_colors)\n",
        "\n",
        "\n",
        "class PlotInternalState(SimpleExtension):\n",
        "    def __init__(self, model, blocks_model, state, features, X, path, **kwargs):\n",
        "        super(PlotInternalState, self).__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.X = X\n",
        "        self.model = model\n",
        "        self.blocks_model = blocks_model\n",
        "        self.internal_state_f = theano.function([features], state, allow_input_downcast=True)\n",
        "        self.internal_state_names = []\n",
        "        for var in state:\n",
        "            self.internal_state_names.append(var.name)\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "        print(\"plotting internal state of network\")\n",
        "        state = self.internal_state_f(self.X)\n",
        "        for ii in range(len(state)):\n",
        "            param_name = self.internal_state_names[ii]\n",
        "            val = state[ii]\n",
        "            filename_safe_name = param_name.replace(' ', '_').replace('/', '-')\n",
        "            base_fname_part1 = self.path + '/state-' + filename_safe_name\n",
        "            base_fname_part2 = '_batch%06d'%self.main_loop.status['iterations_done']\n",
        "            viz.plot_parameter(val, base_fname_part1, base_fname_part2,\n",
        "                title=\"state \" + param_name, n_colors=self.model.n_colors)\n",
        "\n",
        "\n",
        "class PlotMonitors(SimpleExtension):\n",
        "    def __init__(self, path, burn_in_iters=0, **kwargs):\n",
        "        super(PlotMonitors, self).__init__(**kwargs)\n",
        "        self.path = path\n",
        "        self.burn_in_iters = burn_in_iters\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "        print(\"plotting monitors\")\n",
        "        try:\n",
        "            df = self.main_loop.log.to_dataframe()\n",
        "        except AttributeError:\n",
        "            # This starting breaking after a Blocks update.\n",
        "            print(\"Failed to generate monitoring plots due to Blocks interface change.\")\n",
        "            return\n",
        "        iter_number  = df.tail(1).index\n",
        "        # Throw out the first burn_in values\n",
        "        # as the objective is often much larger\n",
        "        # in that period.\n",
        "        if iter_number > self.burn_in_iters:\n",
        "            df = df.loc[self.burn_in_iters:]\n",
        "        cols = [col for col in df.columns if col.startswith(('cost', 'train', 'test'))]\n",
        "        df = df[cols].interpolate(method='linear')\n",
        "\n",
        "        # If we don't have any non-nan dataframes, don't plot\n",
        "        if len(df) == 0:\n",
        "            return\n",
        "        try:\n",
        "            axs = df.interpolate(method='linear').plot(\n",
        "                subplots=True, legend=False, figsize=(5, len(cols)*2))\n",
        "        except TypeError:\n",
        "            # This starting breaking after a different Blocks update.\n",
        "            print(\"Failed to generate monitoring plots due to Blocks interface change.\")\n",
        "            return\n",
        "\n",
        "        for ax, cname in zip(axs, cols):\n",
        "            ax.set_title(cname)\n",
        "        fn = os.path.join(self.path,\n",
        "            'monitors_subplots_batch%06d.png' % self.main_loop.status['iterations_done'])\n",
        "        plt.savefig(fn, bbox_inches='tight')\n",
        "\n",
        "        plt.clf()\n",
        "        df.plot(subplots=False, figsize=(15,10))\n",
        "        plt.gcf().tight_layout()\n",
        "        fn = os.path.join(self.path,\n",
        "            'monitors_batch%06d.png' % self.main_loop.status['iterations_done'])\n",
        "        plt.savefig(fn, bbox_inches='tight')\n",
        "        plt.close('all')\n",
        "\n",
        "\n",
        "class LogLikelihood(SimpleExtension):\n",
        "    def __init__(self, model, test_stream, rescale, num_eval_batches=10000, **kwargs):\n",
        "        \"\"\"\n",
        "        Compute and print log likelihood lower bound on test dataset.\n",
        "        The do() function is called as an extension during training.\n",
        "        \"\"\"\n",
        "        super(LogLikelihood, self).__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.test_stream = test_stream\n",
        "        self.rescale = rescale\n",
        "        self.num_eval_batches = num_eval_batches\n",
        "\n",
        "        features = T.matrix('features', dtype=theano.config.floatX)\n",
        "        cost = self.model.cost(features)\n",
        "\n",
        "        self.L_gap_func = theano.function([features,], cost,\n",
        "            allow_input_downcast=True)\n",
        "\n",
        "    def print_stats(self, L_gap):\n",
        "        larr = np.array(L_gap)\n",
        "        mn = np.mean(larr)\n",
        "        sd = np.std(larr, ddof=1)\n",
        "        stderr = sd / np.sqrt(len(L_gap))\n",
        "\n",
        "        # The log likelihood lower bound, K, is reported for the data after Z-scoring it.\n",
        "        # Z-score rescale is the multiplicative factor by which the data was rescaled, to\n",
        "        # give it standard deviation 1.\n",
        "        print(\"eval batch=%05d  (K-L_null)=%g bits/pix  standard error=%g bits/pix  Z-score rescale %g\"%(\n",
        "            len(L_gap), mn, stderr, self.rescale))\n",
        "\n",
        "    def do(self, callback_name, *args):\n",
        "        L_gap = []\n",
        "        n_colors = self.model.n_colors\n",
        "\n",
        "        Xiter = None\n",
        "        for kk in range(self.num_eval_batches):\n",
        "            try:\n",
        "                X = next(Xiter)[0]\n",
        "            except:\n",
        "                Xiter = self.test_stream.get_epoch_iterator()\n",
        "                X = next(Xiter)[0]\n",
        "\n",
        "            lg = -self.L_gap_func(X)\n",
        "            L_gap.append(lg)\n",
        "\n",
        "            if np.mod(kk, 1000) == 999:\n",
        "                self.print_stats(L_gap)\n",
        "        self.print_stats(L_gap)\n",
        "\n",
        "\n",
        "def decay_learning_rate(iteration, old_value):\n",
        "    # TODO the numbers in this function should not be hard coded\n",
        "\n",
        "    # this is called every epoch\n",
        "    # reduce the learning rate by 10 every 1000 epochs\n",
        "    min_value = 1e-4\n",
        "\n",
        "    decay_rate = np.exp(np.log(0.1)/1000.)\n",
        "    new_value = decay_rate*old_value\n",
        "    if new_value < min_value:\n",
        "        new_value = min_value\n",
        "    print(\"learning rate %g\"%new_value)\n",
        "    return np.float32(new_value)\n"
      ],
      "metadata": {
        "id": "1ChCLOcqPKkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "aeMSk0DZQuf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regression"
      ],
      "metadata": {
        "id": "BjFHkqZoRp-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defines the function approximators\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import theano.tensor as T\n",
        "\n",
        "from blocks.bricks import Activation, MLP, Initializable, application, Identity\n",
        "from blocks.bricks.conv import Convolutional\n",
        "from blocks.initialization import IsotropicGaussian, Constant, Orthogonal\n",
        "\n",
        "# TODO IsotropicGaussian init will be wrong scale for some layers\n",
        "\n",
        "class LeakyRelu(Activation):\n",
        "    @application(inputs=['input_'], outputs=['output'])\n",
        "    def apply(self, input_):\n",
        "        return T.switch(input_ > 0, input_, 0.05*input_)\n",
        "\n",
        "dense_nonlinearity = LeakyRelu()\n",
        "conv_nonlinearity = LeakyRelu()\n",
        "\n",
        "class MultiScaleConvolution(Initializable):\n",
        "    def __init__(self, num_channels, num_filters, spatial_width, num_scales, filter_size, downsample_method='meanout', name=\"\"):\n",
        "        \"\"\"\n",
        "        A brick implementing a single layer in a multi-scale convolutional network.\n",
        "        \"\"\"\n",
        "        super(MultiScaleConvolution, self).__init__()\n",
        "\n",
        "        self.num_scales = num_scales\n",
        "        self.filter_size = filter_size\n",
        "        self.num_filters = num_filters\n",
        "        self.spatial_width = spatial_width\n",
        "        self.downsample_method = downsample_method\n",
        "        self.children = []\n",
        "\n",
        "        print(\"adding MultiScaleConvolution layer\")\n",
        "\n",
        "        # for scale in range(self.num_scales-1, -1, -1):\n",
        "        for scale in range(self.num_scales):\n",
        "            print(\"scale %d\"%scale)\n",
        "            conv_layer = Convolutional(# TODO: Do I need to replace thisactivation=conv_nonlinearity.apply,\n",
        "                filter_size=(filter_size,filter_size), num_filters=num_filters,\n",
        "                num_channels=num_channels, image_size=(spatial_width/2**scale, spatial_width/2**scale),\n",
        "                # assume images are spatially smooth -- in which case output magnitude scales with\n",
        "                # # filter pixels rather than square root of # filter pixels, so initialize\n",
        "                # accordingly.\n",
        "                weights_init=IsotropicGaussian(std=np.sqrt(1./(num_filters))/filter_size**2),\n",
        "                biases_init=Constant(0), border_mode='full', name=name+\"scale%d\"%scale)\n",
        "            self.children.append(conv_layer)\n",
        "\n",
        "    def downsample(self, imgs_in, scale):\n",
        "        \"\"\"\n",
        "        Downsample an image by a factor of 2**scale\n",
        "        \"\"\"\n",
        "        imgs = imgs_in.copy()\n",
        "\n",
        "        if scale == 0:\n",
        "            return imgs\n",
        "\n",
        "        # if self.downsample_method == 'maxout':\n",
        "        #     print \"maxout\",\n",
        "        #     imgs_maxout = downsample.max_pool_2d(imgs.copy(), (2**scale, 2**scale), ignore_border=False)\n",
        "        # else:\n",
        "        #     print \"meanout\",\n",
        "        #     imgs_maxout = self.downsample_mean_pool_2d(imgs.copy(), (2**scale, 2**scale))\n",
        "\n",
        "        num_imgs = imgs.shape[0].astype('int16')\n",
        "        num_layers = imgs.shape[1].astype('int16')\n",
        "        nlx0 = imgs.shape[2].astype('int16')\n",
        "        nlx1 = imgs.shape[3].astype('int16')\n",
        "\n",
        "        scalepow = np.int16(2**scale)\n",
        "\n",
        "        # downsample\n",
        "        imgs = imgs.reshape((num_imgs, num_layers, nlx0/scalepow, scalepow, nlx1/scalepow, scalepow))\n",
        "        imgs = T.mean(imgs, axis=5)\n",
        "        imgs = T.mean(imgs, axis=3)\n",
        "        return imgs\n",
        "\n",
        "    @application\n",
        "    def apply(self, X):\n",
        "\n",
        "        print(\"MultiScaleConvolution apply\")\n",
        "\n",
        "        nsamp = X.shape[0].astype('int16')\n",
        "\n",
        "        Z = 0\n",
        "        # overshoot = (self.filter_size - 1)/2\n",
        "        # TODO: Is this correct\n",
        "        overshoot = int((self.filter_size - 1)/2)\n",
        "        imgs_accum = 0 # accumulate the output image\n",
        "        for scale in range(self.num_scales-1, -1, -1):\n",
        "            # downsample image to appropriate scale\n",
        "            imgs_down = self.downsample(X, scale)\n",
        "            # do a convolutional transformation on it\n",
        "            conv_layer = self.children[scale]\n",
        "            # NOTE this is different than described in the paper, since each conv_layer\n",
        "            # includes a nonlinearity -- it's not just one nonlinearity at the end\n",
        "            imgs_down_conv = conv_layer.apply(imgs_down)\n",
        "\n",
        "            # crop the edge so it's the same size as the input at that scale\n",
        "            print(\"debug\")\n",
        "            print(overshoot)\n",
        "            print(imgs_down_conv)\n",
        "            imgs_down_conv_croppoed = imgs_down_conv[:,:,overshoot:-overshoot,overshoot:-overshoot]\n",
        "            imgs_accum += imgs_down_conv_croppoed\n",
        "\n",
        "            if scale > 0:\n",
        "                # scale up by factor of 2\n",
        "                layer_width = self.spatial_width/2**scale\n",
        "                imgs_accum = imgs_accum.reshape((nsamp, self.num_filters, layer_width, 1, layer_width, 1))\n",
        "                imgs_accum = T.concatenate((imgs_accum, imgs_accum), axis=5)\n",
        "                imgs_accum = T.concatenate((imgs_accum, imgs_accum), axis=3)\n",
        "                imgs_accum = imgs_accum.reshape((nsamp, self.num_filters, layer_width*2, layer_width*2))\n",
        "\n",
        "        return imgs_accum/self.num_scales\n",
        "\n",
        "\n",
        "class MultiLayerConvolution(Initializable):\n",
        "    def __init__(self, n_layers, n_hidden, spatial_width, n_colors, n_scales, filter_size=3):\n",
        "        \"\"\"\n",
        "        A brick implementing a multi-layer, multi-scale convolutional network.\n",
        "        \"\"\"\n",
        "        super(MultiLayerConvolution, self).__init__()\n",
        "\n",
        "        self.children = []\n",
        "        num_channels = n_colors\n",
        "        for ii in range(n_layers):\n",
        "            conv_layer = MultiScaleConvolution(num_channels, n_hidden, spatial_width, n_scales, filter_size, name=\"layer%d_\"%ii)\n",
        "            self.children.append(conv_layer)\n",
        "            num_channels = n_hidden\n",
        "\n",
        "    @application\n",
        "    def apply(self, X):\n",
        "        Z = X\n",
        "        for conv_layer in self.children:\n",
        "            Z = conv_layer.apply(Z)\n",
        "        return Z\n",
        "\n",
        "class MLP_conv_dense(Initializable):\n",
        "    def __init__(self, n_layers_conv, n_layers_dense_lower, n_layers_dense_upper,\n",
        "        n_hidden_conv, n_hidden_dense_lower, n_hidden_dense_lower_output, n_hidden_dense_upper,\n",
        "        spatial_width, n_colors, n_scales, n_temporal_basis):\n",
        "        \"\"\"\n",
        "        The multilayer perceptron, that provides temporal weighting coefficients for mu and sigma\n",
        "        images. This consists of a lower segment with a convolutional MLP, and optionally with a\n",
        "        dense MLP in parallel. The upper segment then consists of a per-pixel dense MLP\n",
        "        (convolutional MLP with 1x1 kernel).\n",
        "        \"\"\"\n",
        "        super(MLP_conv_dense, self).__init__()\n",
        "\n",
        "        self.n_colors = n_colors\n",
        "        self.spatial_width = spatial_width\n",
        "        self.n_hidden_dense_lower = n_hidden_dense_lower\n",
        "        self.n_hidden_dense_lower_output = n_hidden_dense_lower_output\n",
        "        self.n_hidden_conv = n_hidden_conv\n",
        "\n",
        "        ## the lower layers\n",
        "        self.mlp_conv = MultiLayerConvolution(n_layers_conv, n_hidden_conv, spatial_width, n_colors, n_scales)\n",
        "        self.children = [self.mlp_conv]\n",
        "        if n_hidden_dense_lower > 0 and n_layers_dense_lower > 0:\n",
        "            n_input = n_colors*spatial_width**2\n",
        "            n_output = n_hidden_dense_lower_output*spatial_width**2\n",
        "            self.mlp_dense_lower = MLP([dense_nonlinearity] * n_layers_conv,\n",
        "                [n_input] + [n_hidden_dense_lower] * (n_layers_conv-1) + [n_output],\n",
        "                name='MLP dense lower', weights_init=Orthogonal(), biases_init=Constant(0))\n",
        "            self.children.append(self.mlp_dense_lower)\n",
        "        else:\n",
        "            n_hidden_dense_lower_output = 0\n",
        "\n",
        "        ## the upper layers (applied to each pixel independently)\n",
        "        n_output = n_colors*n_temporal_basis*2 # \"*2\" for both mu and sigma\n",
        "        self.mlp_dense_upper = MLP([dense_nonlinearity] * (n_layers_dense_upper-1) + [Identity()],\n",
        "            [n_hidden_conv+n_hidden_dense_lower_output] +\n",
        "            [n_hidden_dense_upper] * (n_layers_dense_upper-1) + [n_output],\n",
        "            name='MLP dense upper', weights_init=Orthogonal(), biases_init=Constant(0))\n",
        "        self.children.append(self.mlp_dense_upper)\n",
        "\n",
        "    @application\n",
        "    def apply(self, X):\n",
        "        \"\"\"\n",
        "        Take in noisy input image and output temporal coefficients for mu and sigma.\n",
        "        \"\"\"\n",
        "        print(X)\n",
        "        Y = self.mlp_conv.apply(X)\n",
        "        Y = Y.dimshuffle(0,2,3,1)\n",
        "        if self.n_hidden_dense_lower > 0:\n",
        "            n_images = X.shape[0].astype('int16')\n",
        "            X = X.reshape((n_images, self.n_colors*self.spatial_width**2))\n",
        "            Y_dense = self.mlp_dense_lower.apply(X)\n",
        "            Y_dense = Y_dense.reshape((n_images, self.spatial_width, self.spatial_width,\n",
        "                self.n_hidden_dense_lower_output))\n",
        "            Y = T.concatenate([Y/T.sqrt(self.n_hidden_conv),\n",
        "                Y_dense/T.sqrt(self.n_hidden_dense_lower_output)], axis=3)\n",
        "        Z = self.mlp_dense_upper.apply(Y)\n",
        "        return Z\n"
      ],
      "metadata": {
        "id": "CmRd7LfARsas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Diffusion Model"
      ],
      "metadata": {
        "id": "iOUOx_YDR2OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is the heart of the algorithm. Implements the objective function and mu\n",
        "and sigma estimators for a Gaussian diffusion probabilistic model\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "from blocks.bricks import application, Initializable, Random\n",
        "\n",
        "# import regression\n",
        "# import util\n",
        "\n",
        "class DiffusionModel(Initializable):\n",
        "    def __init__(self,\n",
        "            spatial_width,\n",
        "            n_colors,\n",
        "            trajectory_length=1000,\n",
        "            n_temporal_basis=10,\n",
        "            n_hidden_dense_lower=500,\n",
        "            n_hidden_dense_lower_output=2,\n",
        "            n_hidden_dense_upper=20,\n",
        "            n_hidden_conv=20,\n",
        "            n_layers_conv=4,\n",
        "            n_layers_dense_lower=4,\n",
        "            n_layers_dense_upper=2,\n",
        "            n_t_per_minibatch=1,\n",
        "            n_scales=1,\n",
        "            step1_beta=0.001,\n",
        "            uniform_noise = 0,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Implements the objective function and mu and sigma estimators for a Gaussian diffusion\n",
        "        probabilistic model, as described in the paper:\n",
        "            Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n",
        "            Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli\n",
        "            International Conference on Machine Learning, 2015\n",
        "\n",
        "        Parameters are as follow:\n",
        "        spatial_width - Spatial_width of training images\n",
        "        n_colors - Number of color channels in training data.\n",
        "        trajectory_length - The number of time steps in the trajectory.\n",
        "        n_temporal_basis - The number of temporal basis functions to capture time-step\n",
        "            dependence of model.\n",
        "        n_hidden_dense_lower - The number of hidden units in each layer of the dense network\n",
        "            in the lower half of the MLP. Set to 0 to make a convolutional-only lower half.\n",
        "        n_hidden_dense_lower_output - The number of outputs *per pixel* from the dense network\n",
        "            in the lower half of the MLP. Total outputs are\n",
        "            n_hidden_dense_lower_output*spatial_width**2.\n",
        "        n_hidden_dense_upper - The number of hidden units per pixel in the upper half of the MLP.\n",
        "        n_hidden_conv - The number of feature layers in the convolutional layers in the lower half\n",
        "            of the MLP.\n",
        "        n_layers_conv - How many convolutional layers to use in the lower half of the MLP.\n",
        "        n_layers_dense_lower - How many dense layers to use in the lower half of the MLP.\n",
        "        n_layers_dense_upper - How many dense layers to use in the upper half of the MLP.\n",
        "        n_t_per_minibatch - When computing objective, how many random time-steps t to evaluate\n",
        "            each minibatch at.\n",
        "        step1_beta - The lower bound on the noise variance of the first diffusion step. This is\n",
        "            the minimum variance of the learned model.\n",
        "        uniform_noise - Add uniform noise between [-uniform_noise/2, uniform_noise/2] to the input.\n",
        "        \"\"\"\n",
        "        super(DiffusionModel, self).__init__()\n",
        "\n",
        "        self.n_t_per_minibatch = n_t_per_minibatch\n",
        "        self.spatial_width = np.int16(spatial_width)\n",
        "        self.n_colors = np.int16(n_colors)\n",
        "        self.n_temporal_basis = n_temporal_basis\n",
        "        self.trajectory_length = trajectory_length\n",
        "        self.uniform_noise = uniform_noise\n",
        "\n",
        "        self.mlp = MLP_conv_dense(\n",
        "            n_layers_conv, n_layers_dense_lower, n_layers_dense_upper,\n",
        "            n_hidden_conv, n_hidden_dense_lower, n_hidden_dense_lower_output, n_hidden_dense_upper,\n",
        "            spatial_width, n_colors, n_scales, n_temporal_basis)\n",
        "        self.temporal_basis = self.generate_temporal_basis(trajectory_length, n_temporal_basis)\n",
        "        self.beta_arr = self.generate_beta_arr(step1_beta)\n",
        "        self.children = [self.mlp]\n",
        "\n",
        "\n",
        "    def generate_beta_arr(self, step1_beta):\n",
        "        \"\"\"\n",
        "        Generate the noise covariances, beta_t, for the forward trajectory.\n",
        "        \"\"\"\n",
        "        # lower bound on beta\n",
        "        min_beta_val = 1e-6\n",
        "        min_beta_values = np.ones((self.trajectory_length,))*min_beta_val\n",
        "        min_beta_values[0] += step1_beta\n",
        "        min_beta = theano.shared(value=min_beta_values.astype(theano.config.floatX),\n",
        "            name='min beta')\n",
        "        # (potentially learned) function for how beta changes with timestep\n",
        "        # TODO add beta_perturb_coefficients to the parameters to be learned\n",
        "        beta_perturb_coefficients_values = np.zeros((self.n_temporal_basis,))\n",
        "        beta_perturb_coefficients = theano.shared(\n",
        "            value=beta_perturb_coefficients_values.astype(theano.config.floatX),\n",
        "            name='beta perturb coefficients')\n",
        "        beta_perturb = T.dot(self.temporal_basis.T, beta_perturb_coefficients)\n",
        "        # baseline behavior of beta with time -- destroy a constant fraction\n",
        "        # of the original data variance each time step\n",
        "        # NOTE 2 below means a fraction ~1/T of the variance will be left at the end of the\n",
        "        # trajectory\n",
        "        beta_baseline = 1./np.linspace(self.trajectory_length, 2., self.trajectory_length)\n",
        "        beta_baseline_offset = logit_np(beta_baseline).astype(theano.config.floatX)\n",
        "        # and the actual beta_t, restricted to be between min_beta and 1-[small value]\n",
        "        beta_arr = T.nnet.sigmoid(beta_perturb + beta_baseline_offset)\n",
        "        beta_arr = min_beta + beta_arr * (1 - min_beta - 1e-5)\n",
        "        beta_arr = beta_arr.reshape((self.trajectory_length, 1))\n",
        "        return beta_arr\n",
        "\n",
        "\n",
        "    def get_t_weights(self, t):\n",
        "        \"\"\"\n",
        "        Generate vector of weights allowing selection of current timestep.\n",
        "        (if t is not an integer, the weights will linearly interpolate)\n",
        "        \"\"\"\n",
        "        n_seg = self.trajectory_length\n",
        "        t_compare = T.arange(n_seg, dtype=theano.config.floatX).reshape((1,n_seg))\n",
        "        diff = abs(T.addbroadcast(t,1) - T.addbroadcast(t_compare,0))\n",
        "        t_weights = T.max(T.join(1, (-diff+1).reshape((n_seg,1)), T.zeros((n_seg,1))), axis=1)\n",
        "        return t_weights.reshape((-1,1))\n",
        "\n",
        "\n",
        "    def get_beta_forward(self, t):\n",
        "        \"\"\"\n",
        "        Get the covariance of the forward diffusion process at timestep\n",
        "        t.\n",
        "        \"\"\"\n",
        "        t_weights = self.get_t_weights(t)\n",
        "        return T.dot(t_weights.T, self.beta_arr)\n",
        "\n",
        "\n",
        "    def get_mu_sigma(self, X_noisy, t):\n",
        "        \"\"\"\n",
        "        Generate mu and sigma for one step in the reverse trajectory,\n",
        "        starting from a minibatch of images X_noisy, and at timestep t.\n",
        "        \"\"\"\n",
        "        Z = self.mlp.apply(X_noisy)\n",
        "        mu_coeff, beta_coeff = self.temporal_readout(Z, t)\n",
        "        # reverse variance is perturbation around forward variance\n",
        "        beta_forward = self.get_beta_forward(t)\n",
        "        # make impact of beta_coeff scaled appropriately with mu_coeff\n",
        "        beta_coeff_scaled = beta_coeff / np.sqrt(self.trajectory_length).astype(theano.config.floatX)\n",
        "        beta_reverse = T.nnet.sigmoid(beta_coeff_scaled + logit(beta_forward))\n",
        "        # # reverse mean is decay towards mu_coeff\n",
        "        # mu = (X_noisy - mu_coeff)*T.sqrt(1. - beta_reverse) + mu_coeff\n",
        "        # reverse mean is a perturbation around the mean under forward\n",
        "        # process\n",
        "\n",
        "\n",
        "        # # DEBUG -- use these lines to test objective is 0 for isotropic Gaussian model\n",
        "        # beta_reverse = beta_forward\n",
        "        # mu_coeff = mu_coeff*0\n",
        "\n",
        "\n",
        "        mu = X_noisy*T.sqrt(1. - beta_forward) + mu_coeff*T.sqrt(beta_forward)\n",
        "        sigma = T.sqrt(beta_reverse)\n",
        "        mu.name = 'mu p'\n",
        "        sigma.name = 'sigma p'\n",
        "        return mu, sigma\n",
        "\n",
        "\n",
        "    def generate_forward_diffusion_sample(self, X_noiseless):\n",
        "        \"\"\"\n",
        "        Corrupt a training image with t steps worth of Gaussian noise, and\n",
        "        return the corrupted image, as well as the mean and covariance of the\n",
        "        posterior q(x^{t-1}|x^t, x^0).\n",
        "        \"\"\"\n",
        "\n",
        "        X_noiseless = X_noiseless.reshape(\n",
        "            (-1, self.n_colors, self.spatial_width, self.spatial_width))\n",
        "\n",
        "        n_images = X_noiseless.shape[0].astype('int16')\n",
        "        rng = Random().theano_rng\n",
        "        # choose a timestep in [1, self.trajectory_length-1].\n",
        "        # note the reverse process is fixed for the very\n",
        "        # first timestep, so we skip it.\n",
        "        # TODO for some reason random_integer is missing from the Blocks\n",
        "        # theano random number generator.\n",
        "        t = T.floor(rng.uniform(size=(1,1), low=1, high=self.trajectory_length,\n",
        "            dtype=theano.config.floatX))\n",
        "        t_weights = self.get_t_weights(t)\n",
        "        N = rng.normal(size=(n_images, self.n_colors, self.spatial_width, self.spatial_width),\n",
        "            dtype=theano.config.floatX)\n",
        "\n",
        "        # noise added this time step\n",
        "        beta_forward = self.get_beta_forward(t)\n",
        "        # decay in noise variance due to original signal this step\n",
        "        alpha_forward = 1. - beta_forward\n",
        "        # compute total decay in the fraction of the variance due to X_noiseless\n",
        "        alpha_arr = 1. - self.beta_arr\n",
        "        alpha_cum_forward_arr = T.extra_ops.cumprod(alpha_arr).reshape((self.trajectory_length,1))\n",
        "        alpha_cum_forward = T.dot(t_weights.T, alpha_cum_forward_arr)\n",
        "        # total fraction of the variance due to noise being mixed in\n",
        "        beta_cumulative = 1. - alpha_cum_forward\n",
        "        # total fraction of the variance due to noise being mixed in one step ago\n",
        "        beta_cumulative_prior_step = 1. - alpha_cum_forward/alpha_forward\n",
        "\n",
        "        # generate the corrupted training data\n",
        "        X_uniformnoise = X_noiseless + (rng.uniform(size=(n_images, self.n_colors, self.spatial_width, self.spatial_width),\n",
        "            dtype=theano.config.floatX)-T.constant(0.5,dtype=theano.config.floatX))*T.constant(self.uniform_noise,dtype=theano.config.floatX)\n",
        "        X_noisy = X_uniformnoise*T.sqrt(alpha_cum_forward) + N*T.sqrt(1. - alpha_cum_forward)\n",
        "\n",
        "        # compute the mean and covariance of the posterior distribution\n",
        "        mu1_scl = T.sqrt(alpha_cum_forward / alpha_forward)\n",
        "        mu2_scl = 1. / T.sqrt(alpha_forward)\n",
        "        cov1 = 1. - alpha_cum_forward/alpha_forward\n",
        "        cov2 = beta_forward / alpha_forward\n",
        "        lam = 1./cov1 + 1./cov2\n",
        "        mu = (\n",
        "                X_uniformnoise * mu1_scl / cov1 +\n",
        "                X_noisy * mu2_scl / cov2\n",
        "            ) / lam\n",
        "        sigma = T.sqrt(1./lam)\n",
        "        sigma = sigma.reshape((1,1,1,1))\n",
        "\n",
        "        mu.name = 'mu q posterior'\n",
        "        sigma.name = 'sigma q posterior'\n",
        "        X_noisy.name = 'X_noisy'\n",
        "        t.name = 't'\n",
        "\n",
        "        return X_noisy, t, mu, sigma\n",
        "\n",
        "\n",
        "    def get_beta_full_trajectory(self):\n",
        "        \"\"\"\n",
        "        Return the cumulative covariance from the entire forward trajectory.\n",
        "        \"\"\"\n",
        "        alpha_arr = 1. - self.beta_arr\n",
        "        beta_full_trajectory = 1. - T.exp(T.sum(T.log(alpha_arr)))\n",
        "        return beta_full_trajectory\n",
        "\n",
        "\n",
        "    def get_negL_bound(self, mu, sigma, mu_posterior, sigma_posterior):\n",
        "        \"\"\"\n",
        "        Compute the lower bound on the log likelihood, as a function of mu and\n",
        "        sigma from the reverse diffusion process, and the posterior mu and\n",
        "        sigma from the forward diffusion process.\n",
        "\n",
        "        Returns the difference between this bound and the log likelihood\n",
        "        under a unit norm isotropic Gaussian. So this function returns how\n",
        "        much better the diffusion model is than an isotropic Gaussian.\n",
        "        \"\"\"\n",
        "\n",
        "        # the KL divergence between model transition and posterior from data\n",
        "        KL = (  T.log(sigma) - T.log(sigma_posterior)\n",
        "                + (sigma_posterior**2 + (mu_posterior-mu)**2)/(2*sigma**2)\n",
        "                - 0.5)\n",
        "        # conditional entropies H_q(x^T|x^0) and H_q(x^1|x^0)\n",
        "        H_startpoint = (0.5*(1 + np.log(2.*np.pi))).astype(theano.config.floatX) + 0.5*T.log(self.beta_arr[0])\n",
        "        H_endpoint = (0.5*(1 + np.log(2.*np.pi))).astype(theano.config.floatX) + 0.5*T.log(self.get_beta_full_trajectory())\n",
        "        H_prior = (0.5*(1 + np.log(2.*np.pi))).astype(theano.config.floatX) + 0.5*T.log(1.)\n",
        "        negL_bound = KL*self.trajectory_length + H_startpoint - H_endpoint + H_prior\n",
        "        # the negL_bound if this was an isotropic Gaussian model of the data\n",
        "        negL_gauss = (0.5*(1 + np.log(2.*np.pi))).astype(theano.config.floatX) + 0.5*T.log(1.)\n",
        "        negL_diff = negL_bound - negL_gauss\n",
        "        L_diff_bits = negL_diff / T.log(2.)\n",
        "        L_diff_bits_avg = L_diff_bits.mean()*self.n_colors\n",
        "        return L_diff_bits_avg\n",
        "\n",
        "\n",
        "    def cost_single_t(self, X_noiseless):\n",
        "        \"\"\"\n",
        "        Compute the lower bound on the log likelihood, given a training minibatch, for a single\n",
        "        randomly chosen timestep.\n",
        "        \"\"\"\n",
        "        X_noisy, t, mu_posterior, sigma_posterior = \\\n",
        "            self.generate_forward_diffusion_sample(X_noiseless)\n",
        "        mu, sigma = self.get_mu_sigma(X_noisy, t)\n",
        "        negL_bound = self.get_negL_bound(mu, sigma, mu_posterior, sigma_posterior)\n",
        "        return negL_bound\n",
        "\n",
        "\n",
        "    def internal_state(self, X_noiseless):\n",
        "        \"\"\"\n",
        "        Return a bunch of the internal state, for monitoring purposes during optimization.\n",
        "        \"\"\"\n",
        "        X_noisy, t, mu_posterior, sigma_posterior = \\\n",
        "            self.generate_forward_diffusion_sample(X_noiseless)\n",
        "        mu, sigma = self.get_mu_sigma(X_noisy, t)\n",
        "        mu_diff = mu-mu_posterior\n",
        "        mu_diff.name = 'mu diff'\n",
        "        logratio = T.log(sigma/sigma_posterior)\n",
        "        logratio.name = 'log sigma ratio'\n",
        "        return [mu_diff, logratio, mu, sigma, mu_posterior, sigma_posterior, X_noiseless, X_noisy]\n",
        "\n",
        "\n",
        "    # \\@application # TODO: Is this needed?\n",
        "    def cost(self, X_noiseless):\n",
        "        \"\"\"\n",
        "        Compute the lower bound on the log likelihood, given a training minibatch.\n",
        "        This will draw a single timestep and compute the cost for that timestep only.\n",
        "        \"\"\"\n",
        "        cost = 0.\n",
        "        for ii in range(self.n_t_per_minibatch):\n",
        "            cost += self.cost_single_t(X_noiseless)\n",
        "        return cost/self.n_t_per_minibatch\n",
        "\n",
        "\n",
        "    def temporal_readout(self, Z, t):\n",
        "        \"\"\"\n",
        "        Go from the top layer of the multilayer perceptron to coefficients for\n",
        "        mu and sigma for each pixel.\n",
        "        Z contains coefficients for spatial basis functions for each pixel for\n",
        "        both mu and sigma.\n",
        "        \"\"\"\n",
        "        n_images = Z.shape[0].astype('int16')\n",
        "        t_weights = self.get_t_weights(t)\n",
        "        Z = Z.reshape((n_images, self.spatial_width, self.spatial_width,\n",
        "            self.n_colors, 2, self.n_temporal_basis))\n",
        "        coeff_weights = T.dot(self.temporal_basis, t_weights)\n",
        "        concat_coeffs = T.dot(Z, coeff_weights)\n",
        "        mu_coeff = concat_coeffs[:,:,:,:,0].dimshuffle(0,3,1,2)\n",
        "        beta_coeff = concat_coeffs[:,:,:,:,1].dimshuffle(0,3,1,2)\n",
        "        return mu_coeff, beta_coeff\n",
        "\n",
        "\n",
        "    def generate_temporal_basis(self, trajectory_length, n_basis):\n",
        "        \"\"\"\n",
        "        Generate the bump basis functions for temporal readout of mu and sigma.\n",
        "        \"\"\"\n",
        "        temporal_basis = np.zeros((trajectory_length, n_basis))\n",
        "        xx = np.linspace(-1, 1, trajectory_length)\n",
        "        x_centers = np.linspace(-1, 1, n_basis)\n",
        "        width = (x_centers[1] - x_centers[0])/2.\n",
        "        for ii in range(n_basis):\n",
        "            temporal_basis[:,ii] = np.exp(-(xx-x_centers[ii])**2 / (2*width**2))\n",
        "        temporal_basis /= np.sum(temporal_basis, axis=1).reshape((-1,1))\n",
        "        temporal_basis = temporal_basis.T\n",
        "\n",
        "        temporal_basis_theano = theano.shared(value=temporal_basis.astype(theano.config.floatX),\n",
        "            name=\"temporal basis\")\n",
        "        return temporal_basis_theano\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNJprXe3Q73z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "06z21DkNPWC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prep model run"
      ],
      "metadata": {
        "id": "Aajpnpf_ptU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/mnist"
      ],
      "metadata": {
        "id": "U8QObJJ9gJ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "pZCCW2DZgR6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export FUEL_DATA_PATH=\"/content/mnist/\"\n",
        "!export data_path=\"/content/mnist/\"\n",
        "# !export FUEL_DATA_PATH=\"/content/mnist/:/second/path/to/my/data\"\n",
        "# ~/.fuelrc\n",
        "!echo \"data_path: \\\"/content/mnist/\\\"\" > ~/.fuelrc\n",
        "!echo \"floatX: int16\" >> ~/.fuelrc"
      ],
      "metadata": {
        "id": "N-XjhnsUgGtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/.fuelrc"
      ],
      "metadata": {
        "id": "Pxux5xPOjyKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuel import config\n",
        "config.data_path = \"/content/mnist/\"\n",
        "config.floatX = 'int16' # https://github.com/mila-iqia/fuel/blob/1d6292dc25e3a115544237e392e61bff6631d23c/tests/transformers/test_transformers.py#L288"
      ],
      "metadata": {
        "id": "Zd3FUuZPkk7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fuel-download mnist -d /content/mnist/"
      ],
      "metadata": {
        "id": "tqmB2h8agm7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fuel-convert mnist -d /content/mnist/ -o /content/mnist/"
      ],
      "metadata": {
        "id": "d58H9AC8kzsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fuel-info /content/mnist/mnist.hdf5"
      ],
      "metadata": {
        "id": "_CndzJ_4k6ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Args"
      ],
      "metadata": {
        "id": "LA0D9Kzdpyuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512\n",
        "        self.lr = 1e-3\n",
        "        self.resume_file = None\n",
        "        self.suffix = ''\n",
        "        self.output_dir = './'\n",
        "        self.ext_every_n = 25\n",
        "        self.model_args = ''\n",
        "        self.dropout_rate = 0\n",
        "        self.dataset = 'MNIST'\n",
        "        self.plot_before_training = False\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
        "\n",
        "args = Args()\n",
        "print(args)"
      ],
      "metadata": {
        "id": "MD5ZNanmb4as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Run"
      ],
      "metadata": {
        "id": "rLUbKPF2p0_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO batches_per_epoch should not be hard coded\n",
        "batches_per_epoch = 500\n",
        "import sys\n",
        "sys.setrecursionlimit(10000000)\n",
        "\n",
        "model_args = eval('dict(' + args.model_args + ')')\n",
        "print(model_args)\n",
        "\n",
        "if args.resume_file is not None:\n",
        "    print(\"Resuming training from \" + args.resume_file)\n",
        "    from blocks.scripts import continue_training\n",
        "    continue_training(args.resume_file)\n",
        "\n",
        "## load the training data\n",
        "if args.dataset == 'MNIST':\n",
        "    from fuel.datasets import MNIST\n",
        "    dataset_train = MNIST(['train'], sources=('features',))\n",
        "    dataset_test = MNIST(['test'], sources=('features',))\n",
        "    n_colors = 1\n",
        "    spatial_width = 28\n",
        "# # elif args.dataset == 'CIFAR10':\n",
        "# #     from fuel.datasets import CIFAR10\n",
        "# #     dataset_train = CIFAR10(['train'], sources=('features',))\n",
        "# #     dataset_test = CIFAR10(['test'], sources=('features',))\n",
        "# #     n_colors = 3\n",
        "# #     spatial_width = 32\n",
        "# # elif args.dataset == 'IMAGENET':\n",
        "# #     from imagenet_data import IMAGENET\n",
        "# #     spatial_width = 128\n",
        "# #     dataset_train = IMAGENET(['train'], width=spatial_width)\n",
        "# #     dataset_test = IMAGENET(['test'], width=spatial_width)\n",
        "# #     n_colors = 3\n",
        "else:\n",
        "    raise ValueError(\"Unknown dataset %s.\"%args.dataset)\n",
        "\n",
        "train_stream = Flatten(DataStream.default_stream(dataset_train,\n",
        "                          iteration_scheme=ShuffledScheme(\n",
        "                              examples=dataset_train.num_examples,\n",
        "                              batch_size=args.batch_size)))\n",
        "test_stream = Flatten(DataStream.default_stream(dataset_test,\n",
        "                          iteration_scheme=ShuffledScheme(\n",
        "                              examples=dataset_test.num_examples,\n",
        "                              batch_size=args.batch_size))\n",
        "                          )\n",
        "\n",
        "shp = next(train_stream.get_epoch_iterator())[0].shape\n",
        "\n",
        "# make the training data 0 mean and variance 1\n",
        "# TODO compute mean and variance on full dataset, not minibatch\n",
        "Xbatch = next(train_stream.get_epoch_iterator())[0]\n",
        "scl = 1./np.sqrt(np.mean((Xbatch-np.mean(Xbatch))**2))\n",
        "shft = -np.mean(Xbatch*scl)\n",
        "# scale is applied before shift\n",
        "train_stream = ScaleAndShift(train_stream, scl, shft)\n",
        "test_stream = ScaleAndShift(test_stream, scl, shft)\n",
        "baseline_uniform_noise = 1./255. # appropriate for MNIST and CIFAR10 Fuel datasets, which are scaled [0,1]\n",
        "uniform_noise = baseline_uniform_noise/scl\n",
        "\n",
        "## initialize the model\n",
        "dpm = DiffusionModel(spatial_width, n_colors, uniform_noise=uniform_noise, **model_args)\n",
        "dpm.initialize()\n",
        "\n",
        "## set up optimization\n",
        "features = T.matrix('features', dtype=theano.config.floatX)\n",
        "cost = dpm.cost(features)\n",
        "\n"
      ],
      "metadata": {
        "id": "3xc3PKdxzYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blocks_model = blocks.model.Model(cost)\n",
        "cg_nodropout = ComputationGraph(cost)\n",
        "if args.dropout_rate > 0:\n",
        "    # DEBUG this triggers an error on my machine\n",
        "    # apply dropout to all the input variables\n",
        "    inputs = VariableFilter(roles=[INPUT])(cg_nodropout.variables)\n",
        "    # dropconnect\n",
        "    # inputs = VariableFilter(roles=[PARAMETER])(cg_nodropout.variables)\n",
        "    cg = apply_dropout(cg_nodropout, inputs, args.dropout_rate)\n",
        "else:\n",
        "    cg = cg_nodropout\n",
        "step_compute = RMSProp(learning_rate=args.lr, max_scaling=1e10)\n",
        "algorithm = GradientDescent(step_rule=CompositeRule([RemoveNotFinite(),\n",
        "    step_compute]),\n",
        "    parameters=cg.parameters, cost=cost)\n",
        "extension_list = []\n",
        "extension_list.append(\n",
        "    SharedVariableModifier(step_compute.learning_rate,\n",
        "        decay_learning_rate,\n",
        "        after_batch=False,\n",
        "        every_n_batches=batches_per_epoch, ))\n",
        "extension_list.append(FinishAfter(after_n_epochs=100001))\n",
        "\n",
        "## logging of test set performance\n",
        "extension_list.append(LogLikelihood(dpm, test_stream, scl,\n",
        "    every_n_batches=args.ext_every_n*batches_per_epoch, before_training=False))\n"
      ],
      "metadata": {
        "id": "qlAHZxAW9llQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## set up logging\n",
        "extension_list.extend([Timing(), Printing()])\n",
        "model_dir = create_log_dir(args, dpm.name + '_' + args.dataset)\n",
        "model_save_name = os.path.join(model_dir, 'model.pkl')\n",
        "extension_list.append(\n",
        "    Checkpoint(model_save_name, every_n_batches=args.ext_every_n*batches_per_epoch, save_separately=['log']))\n",
        "# generate plots\n",
        "extension_list.append(PlotMonitors(model_dir,\n",
        "    every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "test_batch = next(test_stream.get_epoch_iterator())[0]\n",
        "extension_list.append(PlotSamples(dpm, algorithm, test_batch, model_dir,\n",
        "    every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "internal_state = dpm.internal_state(features)\n",
        "train_batch = next(train_stream.get_epoch_iterator())[0]\n",
        "# extension_list.append(\n",
        "#     extensions.PlotInternalState(dpm, blocks_model, internal_state, features, train_batch, model_dir,\n",
        "#         every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "extension_list.append(\n",
        "    PlotParameters(dpm, blocks_model, model_dir,\n",
        "        every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "# extension_list.append(\n",
        "#     extensions.PlotGradients(dpm, blocks_model, algorithm, train_batch, model_dir,\n",
        "#         every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "# # console monitors\n",
        "# # DEBUG -- incorporating train_monitor or test_monitor triggers a large number of\n",
        "# # float64 vs float32 GPU warnings, although monitoring still works. I think this is a Blocks\n",
        "# # bug. Uncomment this code to have more information during debugging/development.\n",
        "# train_monitor_vars = [cost]\n",
        "# norms, grad_norms = util.get_norms(blocks_model, algorithm.gradients)\n",
        "# train_monitor_vars.extend(norms + grad_norms)\n",
        "# train_monitor = TrainingDataMonitoring(\n",
        "#     train_monitor_vars, prefix='train', after_batch=True, before_training=True)\n",
        "# extension_list.append(train_monitor)\n",
        "# test_monitor_vars = [cost]\n",
        "# test_monitor = DataStreamMonitoring(test_monitor_vars, test_stream, prefix='test', before_training=True)\n",
        "# extension_list.append(test_monitor)"
      ],
      "metadata": {
        "id": "T1DSs5N6D7AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## train\n",
        "sys.setrecursionlimit(10000000)\n",
        "main_loop = MainLoop(model=blocks_model, algorithm=algorithm,\n",
        "                     data_stream=train_stream,\n",
        "                     extensions=extension_list)\n",
        "main_loop.run()"
      ],
      "metadata": {
        "id": "Hn4lDYGPJ48b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}