{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dct_diffusion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlQkDVDv/BJk+EldehSgni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/diffusion_models/blob/main/dct_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Theano"
      ],
      "metadata": {
        "id": "4WgkYQS15BKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "6b-Sl-sh5qRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install picklable-itertools==0.1.1"
      ],
      "metadata": {
        "id": "vhAD3zvt_5cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install progressbar2==3.10.0"
      ],
      "metadata": {
        "id": "TZTnZxWT_7o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==3.11"
      ],
      "metadata": {
        "id": "l2pVitJa_9o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install six==1.10.0"
      ],
      "metadata": {
        "id": "bfvzHmlK__7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toolz==0.9.0"
      ],
      "metadata": {
        "id": "uJOB87p8AB2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Theano/Theano.git#egg=theano"
      ],
      "metadata": {
        "id": "3-C1DMO2AD6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/sean-halpin/fuel.git"
      ],
      "metadata": {
        "id": "Jg3BLmOPAFiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --verbose git+https://github.com/mila-iqia/blocks.git"
      ],
      "metadata": {
        "id": "Vd5TVX-hNtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOT_SWd6zDmD"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import theano\n",
        "import theano.tensor as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "from blocks.algorithms import (RMSProp, GradientDescent, CompositeRule,RemoveNotFinite)\n",
        "from blocks.extensions import FinishAfter, Timing, Printing\n",
        "from blocks.extensions.monitoring import (TrainingDataMonitoring,\n",
        "                                          DataStreamMonitoring)\n",
        "from blocks.extensions.saveload import Checkpoint\n",
        "from blocks.extensions.training import SharedVariableModifier\n",
        "from blocks.filter import VariableFilter\n",
        "from blocks.graph import ComputationGraph, apply_dropout\n",
        "from blocks.main_loop import MainLoop\n",
        "import blocks.model\n",
        "from blocks.roles import INPUT, PARAMETER\n",
        "\n",
        "from fuel.streams import DataStream\n",
        "from fuel.schemes import ShuffledScheme\n",
        "from fuel.transformers import Flatten, ScaleAndShift\n"
      ],
      "metadata": {
        "id": "kHlutuUD2Me3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import extensions\n",
        "import model\n",
        "import util"
      ],
      "metadata": {
        "id": "cpOtLgkeO40U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch-size', default=512, type=int,\n",
        "                        help='Batch size')\n",
        "    parser.add_argument('--lr', default=1e-3, type=float,\n",
        "                        help='Initial learning rate. ' + \\\n",
        "                        'Will be decayed until it\\'s 1e-5.')\n",
        "    parser.add_argument('--resume_file', default=None, type=str,\n",
        "                        help='Name of saved model to continue training')\n",
        "    parser.add_argument('--suffix', default='', type=str,\n",
        "                        help='Optional descriptive suffix for model')\n",
        "    parser.add_argument('--output-dir', type=str, default='./',\n",
        "                        help='Output directory to store trained models')\n",
        "    parser.add_argument('--ext-every-n', type=int, default=25,\n",
        "                        help='Evaluate training extensions every N epochs')\n",
        "    parser.add_argument('--model-args', type=str, default='',\n",
        "                        help='Dictionary string to be eval()d containing model arguments.')\n",
        "    parser.add_argument('--dropout_rate', type=float, default=0.,\n",
        "                        help='Rate to use for dropout during training+testing.')\n",
        "    parser.add_argument('--dataset', type=str, default='MNIST',\n",
        "                        help='Name of dataset to use.')\n",
        "    parser.add_argument('--plot_before_training', type=bool, default=False,\n",
        "                        help='Save diagnostic plots at epoch 0, before any training.')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    model_args = eval('dict(' + args.model_args + ')')\n",
        "    print(model_args)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        raise IOError(\"Output directory '%s' does not exist. \"%args.output_dir)\n",
        "    return args, model_args\n",
        "\n",
        "\n",
        "def run():\n",
        "    # TODO batches_per_epoch should not be hard coded\n",
        "    batches_per_epoch = 500\n",
        "    import sys\n",
        "    sys.setrecursionlimit(10000000)\n",
        "\n",
        "    args, model_args = parse_args()\n",
        "\n",
        "    if args.resume_file is not None:\n",
        "        print(\"Resuming training from \" + args.resume_file)\n",
        "        from blocks.scripts import continue_training\n",
        "        continue_training(args.resume_file)\n",
        "\n",
        "    ## load the training data\n",
        "    if args.dataset == 'MNIST':\n",
        "        from fuel.datasets import MNIST\n",
        "        dataset_train = MNIST(['train'], sources=('features',))\n",
        "        dataset_test = MNIST(['test'], sources=('features',))\n",
        "        n_colors = 1\n",
        "        spatial_width = 28\n",
        "    elif args.dataset == 'CIFAR10':\n",
        "        from fuel.datasets import CIFAR10\n",
        "        dataset_train = CIFAR10(['train'], sources=('features',))\n",
        "        dataset_test = CIFAR10(['test'], sources=('features',))\n",
        "        n_colors = 3\n",
        "        spatial_width = 32\n",
        "    elif args.dataset == 'IMAGENET':\n",
        "        from imagenet_data import IMAGENET\n",
        "        spatial_width = 128\n",
        "        dataset_train = IMAGENET(['train'], width=spatial_width)\n",
        "        dataset_test = IMAGENET(['test'], width=spatial_width)\n",
        "        n_colors = 3\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset %s.\"%args.dataset)\n",
        "\n",
        "    train_stream = Flatten(DataStream.default_stream(dataset_train,\n",
        "                              iteration_scheme=ShuffledScheme(\n",
        "                                  examples=dataset_train.num_examples,\n",
        "                                  batch_size=args.batch_size)))\n",
        "    test_stream = Flatten(DataStream.default_stream(dataset_test,\n",
        "                             iteration_scheme=ShuffledScheme(\n",
        "                                 examples=dataset_test.num_examples,\n",
        "                                 batch_size=args.batch_size))\n",
        "                             )\n",
        "\n",
        "    shp = next(train_stream.get_epoch_iterator())[0].shape\n",
        "\n",
        "    # make the training data 0 mean and variance 1\n",
        "    # TODO compute mean and variance on full dataset, not minibatch\n",
        "    Xbatch = next(train_stream.get_epoch_iterator())[0]\n",
        "    scl = 1./np.sqrt(np.mean((Xbatch-np.mean(Xbatch))**2))\n",
        "    shft = -np.mean(Xbatch*scl)\n",
        "    # scale is applied before shift\n",
        "    train_stream = ScaleAndShift(train_stream, scl, shft)\n",
        "    test_stream = ScaleAndShift(test_stream, scl, shft)\n",
        "    baseline_uniform_noise = 1./255. # appropriate for MNIST and CIFAR10 Fuel datasets, which are scaled [0,1]\n",
        "    uniform_noise = baseline_uniform_noise/scl\n",
        "\n",
        "    ## initialize the model\n",
        "    dpm = model.DiffusionModel(spatial_width, n_colors, uniform_noise=uniform_noise, **model_args)\n",
        "    dpm.initialize()\n",
        "\n",
        "    ## set up optimization\n",
        "    features = T.matrix('features', dtype=theano.config.floatX)\n",
        "    cost = dpm.cost(features)\n",
        "    blocks_model = blocks.model.Model(cost)\n",
        "    cg_nodropout = ComputationGraph(cost)\n",
        "    if args.dropout_rate > 0:\n",
        "        # DEBUG this triggers an error on my machine\n",
        "        # apply dropout to all the input variables\n",
        "        inputs = VariableFilter(roles=[INPUT])(cg_nodropout.variables)\n",
        "        # dropconnect\n",
        "        # inputs = VariableFilter(roles=[PARAMETER])(cg_nodropout.variables)\n",
        "        cg = apply_dropout(cg_nodropout, inputs, args.dropout_rate)\n",
        "    else:\n",
        "        cg = cg_nodropout\n",
        "    step_compute = RMSProp(learning_rate=args.lr, max_scaling=1e10)\n",
        "    algorithm = GradientDescent(step_rule=CompositeRule([RemoveNotFinite(),\n",
        "        step_compute]),\n",
        "        parameters=cg.parameters, cost=cost)\n",
        "    extension_list = []\n",
        "    extension_list.append(\n",
        "        SharedVariableModifier(step_compute.learning_rate,\n",
        "            extensions.decay_learning_rate,\n",
        "            after_batch=False,\n",
        "            every_n_batches=batches_per_epoch, ))\n",
        "    extension_list.append(FinishAfter(after_n_epochs=100001))\n",
        "\n",
        "    ## logging of test set performance\n",
        "    extension_list.append(extensions.LogLikelihood(dpm, test_stream, scl,\n",
        "        every_n_batches=args.ext_every_n*batches_per_epoch, before_training=False))\n",
        "\n",
        "    ## set up logging\n",
        "    extension_list.extend([Timing(), Printing()])\n",
        "    model_dir = util.create_log_dir(args, dpm.name + '_' + args.dataset)\n",
        "    model_save_name = os.path.join(model_dir, 'model.pkl')\n",
        "    extension_list.append(\n",
        "        Checkpoint(model_save_name, every_n_batches=args.ext_every_n*batches_per_epoch, save_separately=['log']))\n",
        "    # generate plots\n",
        "    extension_list.append(extensions.PlotMonitors(model_dir,\n",
        "        every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "    test_batch = next(test_stream.get_epoch_iterator())[0]\n",
        "    extension_list.append(extensions.PlotSamples(dpm, algorithm, test_batch, model_dir,\n",
        "        every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "    internal_state = dpm.internal_state(features)\n",
        "    train_batch = next(train_stream.get_epoch_iterator())[0]\n",
        "    # extension_list.append(\n",
        "    #     extensions.PlotInternalState(dpm, blocks_model, internal_state, features, train_batch, model_dir,\n",
        "    #         every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "    extension_list.append(\n",
        "        extensions.PlotParameters(dpm, blocks_model, model_dir,\n",
        "            every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "    # extension_list.append(\n",
        "    #     extensions.PlotGradients(dpm, blocks_model, algorithm, train_batch, model_dir,\n",
        "    #         every_n_batches=args.ext_every_n*batches_per_epoch, before_training=args.plot_before_training))\n",
        "    # # console monitors\n",
        "    # # DEBUG -- incorporating train_monitor or test_monitor triggers a large number of\n",
        "    # # float64 vs float32 GPU warnings, although monitoring still works. I think this is a Blocks\n",
        "    # # bug. Uncomment this code to have more information during debugging/development.\n",
        "    # train_monitor_vars = [cost]\n",
        "    # norms, grad_norms = util.get_norms(blocks_model, algorithm.gradients)\n",
        "    # train_monitor_vars.extend(norms + grad_norms)\n",
        "    # train_monitor = TrainingDataMonitoring(\n",
        "    #     train_monitor_vars, prefix='train', after_batch=True, before_training=True)\n",
        "    # extension_list.append(train_monitor)\n",
        "    # test_monitor_vars = [cost]\n",
        "    # test_monitor = DataStreamMonitoring(test_monitor_vars, test_stream, prefix='test', before_training=True)\n",
        "    # extension_list.append(test_monitor)\n",
        "\n",
        "    ## train\n",
        "    sys.setrecursionlimit(10000000)\n",
        "    main_loop = MainLoop(model=blocks_model, algorithm=algorithm,\n",
        "                         data_stream=train_stream,\n",
        "                         extensions=extension_list)\n",
        "    main_loop.run()\n",
        "\n",
        "run()\n"
      ],
      "metadata": {
        "id": "3xc3PKdxzYVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}