{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional DCGAN MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/diffusion_models/blob/main/GAN_implementations/Conditional_DCGAN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4E109s868zM"
      },
      "source": [
        "# Conditional Deep Convolutional Generative Adversarial Networks\n",
        "\n",
        "Implement conditional GAN as mentioned in [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) paper.\n",
        "\n",
        "We will use DCGAN architecture and modify for taking conditional input labels.\n",
        "\n",
        "\n",
        "### Let's start by imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yab643D6XxP"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REVYXxAndCZ-"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3hzCeX4fC1l"
      },
      "source": [
        "# use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                      else \"cpu\")\n",
        "print(f\"Using {DEVICE} backend\")\n",
        "\n",
        "# Training data, you can choose MNIST or Fashion MNIST\n",
        "DATASET_NAME = \"Cifar\" #@param [\"MNIST\", \"Fashion MNIST\", \"Celeba\", \"Cifar\"]\n",
        "# batch size for training models \n",
        "# Change multiple of 16 only, else modify below code\n",
        "BATCH_SIZE = 128 #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npKgap6npJaT"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "We will use Fashion MNIST and MNIST dataset available as part of torchvision module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4EyOEgUlZE3"
      },
      "source": [
        "%%capture\n",
        "from torchvision.transforms.transforms import Resize\n",
        "# define transform to convert images to tensors and normalize\n",
        "# after normalization images will be in range [-1, 1] with mean zero.\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5], [0.5])\n",
        "                               ])\n",
        "if DATASET_NAME == \"MNIST\":\n",
        "  # download and load training set of MNIST\n",
        "  train_data = datasets.MNIST(\"./mnist\", train=True, download=True, transform = transform)\n",
        "elif DATASET_NAME == \"Fashion MNIST\":\n",
        "  # download and load training set of Fashion MNIST\n",
        "  train_data = datasets.FashionMNIST(\"./fmnist\", train=True, download=True, transform = transform)\n",
        "elif DATASET_NAME == \"Celeba\":\n",
        "  transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Grayscale(num_output_channels=1),\n",
        "                                transforms.Resize((32,32)),\n",
        "                                transforms.Normalize([0.5], [0.5])\n",
        "                               ])\n",
        "  # download\n",
        "  train_data = datasets.CelebA(\"./CelebA\", download=True, transform = transform)\n",
        "elif DATASET_NAME == \"Cifar\":\n",
        "  transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Grayscale(num_output_channels=1),\n",
        "                                transforms.Resize((28,28)),\n",
        "                                transforms.Normalize([0.5], [0.5])\n",
        "                               ])\n",
        "  train_data = datasets.CIFAR10(\"./CIFAR10\", download=True, transform = transform)\n",
        "  # download\n",
        "else:\n",
        "  ValueError(f\"Please select valid dataset, {DATASET_NAME} is not supported\")\n",
        "# create generator data loader with given batch size\n",
        "dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Vdj6cU52zN"
      },
      "source": [
        "# for checking data, lets create iterator\n",
        "# We won't use this iterator for training purpose\n",
        "dataiter = iter(dataloader)\n",
        "images, labels = next(dataiter)\n",
        "# MNIST image size is 1*28*28\n",
        "img_size = images.shape[2]\n",
        "print(images.shape)\n",
        "# Create images grid of 16 rows, batch size need to be multiple of 16\n",
        "# for batch size of 128, 128 images will be arranged in 8*16 grid \n",
        "torchvision.utils.save_image(images, f\"{DATASET_NAME}_input_grid.jpg\", nrow=16, padding=0, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLRSL_OeskXU"
      },
      "source": [
        "# Load saved training images grid and visualize using matplotlib\n",
        "# Figure of size 16*(Batch_size/16)\n",
        "plt.figure(figsize=(16, BATCH_SIZE/16))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "# using _ for avoiding extra outputs\n",
        "_ = plt.imshow(Image.open(f\"{DATASET_NAME}_input_grid.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW4E-IV4y3Qn"
      },
      "source": [
        "## Discriminator Model ( D )\n",
        "\n",
        "Discriminator is a binary classification model, which predicts if given image is generated one or taken from training data.\n",
        "\n",
        "Discriminator model will take a image and a class label. We will reshape class label to shape (batch_size, num_labels, 28, 28) and channel corresponding to image labels will have all ones and other all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAU5B0HfVzs"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"\"\" D(x) \"\"\"\n",
        "  def __init__(self):\n",
        "    # initalize super module\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    # creating layer for image input , input size : (batch_size, 1, 28, 28)\n",
        "    self.layer_x = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32,\n",
        "                                           kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 nn.LeakyReLU(0.2, inplace=True),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                )\n",
        "                                 \n",
        "    # creating layer for label input, input size : (batch_size, 10, 28, 28)\n",
        "    self.layer_y = nn.Sequential(nn.Conv2d(in_channels=10, out_channels=32,\n",
        "                                           kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 nn.LeakyReLU(0.2, inplace=True),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 )\n",
        "    \n",
        "    # layer for concat of image layer and label layer, input size : (batch_size, 64, 14, 14)\n",
        "    self.layer_xy = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128,\n",
        "                                            kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.BatchNorm2d(128),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.Conv2d(in_channels=128, out_channels=256,\n",
        "                                         kernel_size=3, stride=2, padding=0, bias=False),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               nn.BatchNorm2d(256),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               # Notice in below layer, we are using out channels as 1, we don't need to use Linear layer\n",
        "                               # Same is recommended in DCGAN paper also\n",
        "                               nn.Conv2d(in_channels=256, out_channels=1,\n",
        "                                         kernel_size=3, stride=1, padding=0, bias=False),\n",
        "                               # out size : (batch_size, 1, 1, 1)\n",
        "                               # sigmoid layer to convert in [0,1] range\n",
        "                               nn.Sigmoid()\n",
        "                               )\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    # size of x : (batch_size, 1, 28, 28)\n",
        "    x = self.layer_x(x)\n",
        "    # size of x : (batch_size, 32, 14, 14)\n",
        "    \n",
        "    # size of y : (batch_size, 10, 28, 28)\n",
        "    y = self.layer_y(y)\n",
        "    # size of y : (batch_size, 32, 14, 14)\n",
        "    \n",
        "    # concat image layer and label layer output\n",
        "    xy = torch.cat([x,y], dim=1)\n",
        "    # size of xy : (batch_size, 64, 14, 14)\n",
        "    xy = self.layer_xy(xy)\n",
        "    # size of xy : (batch_size, 1, 1, 1)\n",
        "    xy = xy.view(xy.shape[0], -1)\n",
        "    # size of xy : (batch_size, 1)\n",
        "    return xy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SVZbnxzpLL0"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator().to(DEVICE)\n",
        "print(netD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zm6hsLk4kHv"
      },
      "source": [
        "## Generator Model ( G )\n",
        "\n",
        "Aim of the generator is to fool the discriminator model. Generator will take random noise ( latent vector) z and label for which image need to be generated. Label will be passed as onehot encoding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp6MPki8jy8z"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\" G(z) \"\"\"\n",
        "  def __init__(self, input_size=100):\n",
        "    # initalize super module\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    # noise z input layer : (batch_size, 100, 1, 1)\n",
        "    self.layer_x = nn.Sequential(nn.ConvTranspose2d(in_channels=100, out_channels=128, kernel_size=3,\n",
        "                                                  stride=1, padding=0, bias=False),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.ReLU(),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                )\n",
        "    \n",
        "    # label input layer : (batch_size, 10, 1, 1)\n",
        "    self.layer_y = nn.Sequential(nn.ConvTranspose2d(in_channels=10, out_channels=128, kernel_size=3,\n",
        "                                                  stride=1, padding=0, bias=False),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.ReLU(),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                )\n",
        "    \n",
        "    # noise z and label concat input layer : (batch_size, 256, 3, 3)\n",
        "    self.layer_xy = nn.Sequential(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3,\n",
        "                                                  stride=2, padding=0, bias=False),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.BatchNorm2d(128),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.ReLU(),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4,\n",
        "                                                  stride=2, padding=1, bias=False),\n",
        "                               # out size : (batch_size, 64, 14, 14)\n",
        "                               nn.BatchNorm2d(64),\n",
        "                               # out size : (batch_size, 64, 14, 14)\n",
        "                               nn.ReLU(),\n",
        "                               # out size : (batch_size, 64, 14, 14)\n",
        "                               nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4,\n",
        "                                                  stride=2, padding=1, bias=False),\n",
        "                               # out size : (batch_size, 1, 28, 28)\n",
        "                               nn.Tanh())\n",
        "                               # out size : (batch_size, 1, 28, 28)\n",
        "    \n",
        "  def forward(self, x, y):\n",
        "    # x size : (batch_size, 100)\n",
        "    x = x.view(x.shape[0], x.shape[1], 1, 1)\n",
        "    # x size : (batch_size, 100, 1, 1)\n",
        "    x = self.layer_x(x)\n",
        "    # x size : (batch_size, 128, 3, 3)\n",
        "    \n",
        "    # y size : (batch_size, 10)\n",
        "    y = y.view(y.shape[0], y.shape[1], 1, 1)\n",
        "    # y size : (batch_size, 100, 1, 1)\n",
        "    y = self.layer_y(y)\n",
        "    # y size : (batch_size, 128, 3, 3)\n",
        "    \n",
        "    # concat x and y \n",
        "    xy = torch.cat([x,y], dim=1)\n",
        "    # xy size : (batch_size, 256, 3, 3)\n",
        "    xy = self.layer_xy(xy)\n",
        "    # xy size : (batch_size, 1, 28, 28)\n",
        "    return xy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLNGGbjrnq5u"
      },
      "source": [
        "# Create the Generator\n",
        "netG = Generator().to(DEVICE)\n",
        "print(netG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6YXrnydFPc-"
      },
      "source": [
        "## Weight Initalization \n",
        "From the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbM4ilemFK8T"
      },
      "source": [
        "# custom weights initialization\n",
        "def weights_init(net):\n",
        "    classname = net.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(net.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(net.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJvl9TbIvaR"
      },
      "source": [
        "# randomly initialize all weights to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "netG.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_fmu5lBUdPv"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "\n",
        "\n",
        "*   Value of beta1 hyperparameter in Adam optimizer has huge impact on stability of generator and DCGAN paper recommend 0.5 value.\n",
        "*   Recommended learning rate for Adam is 0.0002."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7ppD6ZOoQP-"
      },
      "source": [
        "# number of training epochs\n",
        "NUM_EPOCH =  50#@param {type:\"integer\"}\n",
        "# size of latent vector z\n",
        "size_z = 100\n",
        "# number of discriminator steps for each generator step\n",
        "Ksteps = 1 #@param {type:\"integer\"}\n",
        "# learning rate of adam\n",
        "# DCGAN recommend 0.0002 lr\n",
        "Adam_lr = 0.0002 #@param {type:\"number\"}\n",
        "# DCGAN recommend 0.5\n",
        "Adam_beta1 = 0.5 #@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4I_fj3knUrW"
      },
      "source": [
        "# We calculate Binary cross entropy loss\n",
        "criterion = nn.BCELoss()\n",
        "# Adam optimizer for generator \n",
        "optimizerG = torch.optim.Adam(netG.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))\n",
        "# Adam optimizer for discriminator \n",
        "optimizerD = torch.optim.Adam(netD.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyhZNliUrgCD"
      },
      "source": [
        "# labels for training images x for Discriminator training\n",
        "labels_real = torch.ones((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# labels for generated images G(z) for Discriminator training\n",
        "labels_fake = torch.zeros((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# Fix noise for testing generator and visualization\n",
        "z_test = torch.randn(100, size_z).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXzZJazbbGMY"
      },
      "source": [
        "# convert labels to onehot encoding\n",
        "onehot = torch.zeros(10, 10).scatter_(1, torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1)\n",
        "# reshape labels to image size, with number of labels as channel\n",
        "fill = torch.zeros([10, 10, img_size, img_size])\n",
        "#channel corresponding to label will be set one and all other zeros\n",
        "for i in range(10):\n",
        "    fill[i, i, :, :] = 1\n",
        "# create labels for testing generator\n",
        "test_y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]*10).type(torch.LongTensor)\n",
        "# convert to one hot encoding\n",
        "test_Gy = onehot[test_y].to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IllIZmPCpU30",
        "outputId": "b8452989-9b63-4d8c-a10f-7bbe5dbc7c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List of values, which will be used for plotting purpose\n",
        "D_losses = []\n",
        "G_losses = []\n",
        "Dx_values = []\n",
        "DGz_values = []\n",
        "\n",
        "# number of training steps done on discriminator \n",
        "step = 0\n",
        "for epoch in range(NUM_EPOCH):\n",
        "  epoch_D_losses = []\n",
        "  epoch_G_losses = []\n",
        "  epoch_Dx = []\n",
        "  epoch_DGz = []\n",
        "  # iterate through data loader generator object\n",
        "  for images, y_labels in dataloader:\n",
        "    step += 1\n",
        "    ############################\n",
        "    # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "    ###########################\n",
        "    # images will be send to gpu, if cuda available\n",
        "    x = images.to(DEVICE)\n",
        "    # preprocess labels for feeding as y input\n",
        "    # D_y shape will be (batch_size, 10, 28, 28)\n",
        "    D_y = fill[y_labels].to(DEVICE)\n",
        "    # forward pass D(x)\n",
        "    x_preds = netD(x, D_y)\n",
        "    # calculate loss log(D(x))\n",
        "    D_x_loss = criterion(x_preds, labels_real)\n",
        "    \n",
        "    # create latent vector z from normal distribution \n",
        "    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # create random y labels for generator\n",
        "    y_gen = (torch.rand(BATCH_SIZE, 1)*10).type(torch.LongTensor).squeeze()\n",
        "    # convert genarator labels to onehot\n",
        "    G_y = onehot[y_gen].to(DEVICE)\n",
        "    # preprocess labels for feeding as y input in D\n",
        "    # DG_y shape will be (batch_size, 10, 28, 28)\n",
        "    DG_y = fill[y_gen].to(DEVICE)\n",
        "    \n",
        "    # generate image\n",
        "    fake_image = netG(z, G_y)\n",
        "    # calculate D(G(z)), fake or not\n",
        "    z_preds = netD(fake_image.detach(), DG_y)\n",
        "    # loss log(1 - D(G(z)))\n",
        "    D_z_loss = criterion(z_preds, labels_fake)\n",
        "    \n",
        "    # total loss = log(D(x)) + log(1 - D(G(z)))\n",
        "    D_loss = D_x_loss + D_z_loss\n",
        "    \n",
        "    # save values for plots\n",
        "    epoch_D_losses.append(D_loss.item())\n",
        "    epoch_Dx.append(x_preds.mean().item())\n",
        "    \n",
        "    # zero accumalted grads\n",
        "    netD.zero_grad()\n",
        "    # do backward pass\n",
        "    D_loss.backward()\n",
        "    # update discriminator model\n",
        "    optimizerD.step()\n",
        "    \n",
        "    ############################\n",
        "    # Update G network: maximize log(D(G(z)))\n",
        "    ###########################\n",
        "        \n",
        "    # if Ksteps of Discriminator training are done, update generator\n",
        "    if step % Ksteps == 0:\n",
        "      # As we done one step of discriminator, again calculate D(G(z))\n",
        "      z_out = netD(fake_image, DG_y)\n",
        "      # loss log(D(G(z)))\n",
        "      G_loss = criterion(z_out, labels_real)\n",
        "      # save values for plots\n",
        "      epoch_DGz.append(z_out.mean().item())\n",
        "      epoch_G_losses.append(G_loss)\n",
        "      \n",
        "      # zero accumalted grads\n",
        "      netG.zero_grad()\n",
        "      # do backward pass\n",
        "      G_loss.backward()\n",
        "      # update generator model\n",
        "      optimizerG.step()\n",
        "  else:\n",
        "    # calculate average value for one epoch\n",
        "    D_losses.append(sum(epoch_D_losses)/len(epoch_D_losses))\n",
        "    G_losses.append(sum(epoch_G_losses)/len(epoch_G_losses))\n",
        "    Dx_values.append(sum(epoch_Dx)/len(epoch_Dx))\n",
        "    DGz_values.append(sum(epoch_DGz)/len(epoch_DGz))\n",
        "    \n",
        "    print(f\" Epoch {epoch+1}/{NUM_EPOCH} Discriminator Loss {D_losses[-1]:.3f} Generator Loss {G_losses[-1]:.3f}\"\n",
        "         + f\" D(x) {Dx_values[-1]:.3f} D(G(x)) {DGz_values[-1]:.3f}\")\n",
        "    \n",
        "    # Generating images after each epoch and saving\n",
        "    # set generator to evaluation mode\n",
        "    netG.eval()\n",
        "    with torch.no_grad():\n",
        "      # forward pass of G and generated image\n",
        "      fake_test = netG(z_test, test_Gy).cpu()\n",
        "      # save images in grid of 10 * 10\n",
        "      torchvision.utils.save_image(fake_test, f\"{DATASET_NAME}_epoch_{epoch+1}.jpg\", nrow=10, padding=0, normalize=True)\n",
        "    # set generator to training mode\n",
        "    netG.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 37/50 Discriminator Loss 0.582 Generator Loss 2.525 D(x) 0.791 D(G(x)) 0.144\n",
            " Epoch 38/50 Discriminator Loss 0.518 Generator Loss 2.526 D(x) 0.803 D(G(x)) 0.134\n",
            " Epoch 39/50 Discriminator Loss 0.582 Generator Loss 2.567 D(x) 0.796 D(G(x)) 0.138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD0jTPLKZzTu"
      },
      "source": [
        "## Results\n",
        "\n",
        "After 20 epoch training, we are able to generate quite good images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEbWD88gZ2Rw"
      },
      "source": [
        "# Load saved generated images grid and visualize using matplotlib\n",
        "# Figure of size 10*10\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Generated Images\")\n",
        "# using _ for avoiding extra outputs\n",
        "_ = plt.imshow(Image.open(f\"{DATASET_NAME}_epoch_{NUM_EPOCH}.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z7pzLWnaz9l"
      },
      "source": [
        "### Animation of generated images after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNBURp1oMyX3"
      },
      "source": [
        "%%capture\n",
        "# using capture magic for avoiding extra outputs\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "# load all images\n",
        "ims = [[plt.imshow(Image.open(f\"{DATASET_NAME}_epoch_{i+1}.jpg\"), animated=True)] for i in range(NUM_EPOCH)]\n",
        "# create animation \n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jsVJBcPNj0_"
      },
      "source": [
        "# showing animation using Ipython HTML\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0rp8uq48Gjr"
      },
      "source": [
        "ani.save('FMNIST_animation.gif', writer='imagemagick', fps=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urOunOeud4a5"
      },
      "source": [
        "\n",
        "\n",
        "### Plot for Discriminator and Generator loss over the epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCMwKVoT4x5A"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator and Generator loss during Training\")\n",
        "# plot Discriminator and generator loss\n",
        "plt.plot(D_losses,label=\"D Loss\")\n",
        "plt.plot(G_losses,label=\"G Loss\")\n",
        "# get plot axis\n",
        "ax = plt.gca()\n",
        "# remove right and top spine\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "# add labels and create legend\n",
        "plt.xlabel(\"num_epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}