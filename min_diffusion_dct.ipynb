{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "min_diffusion_dct.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3YLy7Q/2mofNj84kxSYfV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/diffusion_models/blob/main/min_diffusion_dct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/images/"
      ],
      "metadata": {
        "id": "Llm5lF5Rx32V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "dTzJUI2DIrk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Nvg8EauCEl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extremely Minimalistic Implementation of DDPM\n",
        "\n",
        "https://arxiv.org/abs/2006.11239\n",
        "\n",
        "Everything is self contained. (Except for pytorch and torchvision... of course)\n",
        "\n",
        "run it with `python superminddpm.py`\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "\n",
        "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns pre-computed schedules for DDPM sampling, training process.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "\n",
        "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    sqrt_beta_t = torch.sqrt(beta_t)\n",
        "    alpha_t = 1 - beta_t\n",
        "    log_alpha_t = torch.log(alpha_t)\n",
        "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
        "\n",
        "    sqrtab = torch.sqrt(alphabar_t)\n",
        "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
        "\n",
        "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
        "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
        "\n",
        "    return {\n",
        "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
        "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
        "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
        "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
        "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
        "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
        "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
        "    }\n",
        "\n",
        "\n",
        "blk = lambda ic, oc: nn.Sequential(\n",
        "    nn.Conv2d(ic, oc, 7, padding=3),\n",
        "    nn.BatchNorm2d(oc),\n",
        "    nn.LeakyReLU(),\n",
        ")\n",
        "\n",
        "\n",
        "class DummyEpsModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This should be unet-like, but let's don't think about the model too much :P\n",
        "    Basically, any universal R^n -> R^n model should work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channel: int) -> None:\n",
        "        super(DummyEpsModel, self).__init__()\n",
        "        self.conv = nn.Sequential(  # with batchnorm\n",
        "            blk(n_channel, 64),\n",
        "            blk(64, 128),\n",
        "            blk(128, 256),\n",
        "            blk(256, 512),\n",
        "            blk(512, 256),\n",
        "            blk(256, 128),\n",
        "            blk(128, 64),\n",
        "            nn.Conv2d(64, n_channel, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t) -> torch.Tensor:\n",
        "        # Lets think about using t later. In the paper, they used Tr-like positional embeddings.\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eps_model: nn.Module,\n",
        "        betas: Tuple[float, float],\n",
        "        n_T: int,\n",
        "        criterion: nn.Module = nn.MSELoss(),\n",
        "    ) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.eps_model = eps_model\n",
        "\n",
        "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.\n",
        "        This implements Algorithm 1 in the paper.\n",
        "        \"\"\"\n",
        "\n",
        "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
        "            x.device\n",
        "        )  # t ~ Uniform(0, n_T)\n",
        "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "\n",
        "        x_t = (\n",
        "            self.sqrtab[_ts, None, None, None] * x\n",
        "            + self.sqrtmab[_ts, None, None, None] * eps\n",
        "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
        "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
        "\n",
        "        return self.criterion(eps, self.eps_model(x_t, _ts / self.n_T))\n",
        "\n",
        "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
        "\n",
        "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
        "\n",
        "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
        "        for i in range(self.n_T, 0, -1):\n",
        "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
        "            eps = self.eps_model(x_i, i / self.n_T)\n",
        "            x_i = (\n",
        "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
        "                + self.sqrt_beta_t[i] * z\n",
        "            )\n",
        "\n",
        "        return x_i\n",
        "\n",
        "\n",
        "def train_mnist(n_epoch: int = 1, device=\"cuda:0\") -> None:\n",
        "\n",
        "    ddpm = DDPM(eps_model=DummyEpsModel(1), betas=(1e-4, 0.02), n_T=1000)\n",
        "    ddpm.to(device)\n",
        "\n",
        "    tf = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))]\n",
        "    )\n",
        "\n",
        "    dataset = MNIST(\n",
        "        \"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=tf,\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=20)\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=2e-4)\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        ddpm.train()\n",
        "\n",
        "        pbar = tqdm(dataloader)\n",
        "        loss_ema = None\n",
        "        for x, _ in pbar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_ema is None:\n",
        "                loss_ema = loss.item()\n",
        "            else:\n",
        "                loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
        "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(16, (1, 28, 28), device)\n",
        "            grid = make_grid(xh, nrow=4)\n",
        "            save_image(grid, f\"/content/images/ddpm_sample_{i}.png\")\n",
        "\n",
        "            # save model\n",
        "            torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_mnist()"
      ],
      "metadata": {
        "id": "3qhBGfKtuIJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR"
      ],
      "metadata": {
        "id": "6zAXBScMIxVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNet"
      ],
      "metadata": {
        "id": "A9K3x1nQI2BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Unet Structure.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Conv3(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.is_res = is_res\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.main(x)\n",
        "        if self.is_res:\n",
        "            x = x + self.conv(x)\n",
        "            return x / 1.414\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super(UnetDown, self).__init__()\n",
        "        layers = [Conv3(in_channels, out_channels), nn.MaxPool2d(2)]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super(UnetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
        "            Conv3(out_channels, out_channels),\n",
        "            Conv3(out_channels, out_channels),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TimeSiren(nn.Module):\n",
        "    def __init__(self, emb_dim: int) -> None:\n",
        "        super(TimeSiren, self).__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(1, emb_dim, bias=False)\n",
        "        self.lin2 = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.view(-1, 1)\n",
        "        x = torch.sin(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NaiveUnet(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, n_feat: int = 256) -> None:\n",
        "        super(NaiveUnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.n_feat = n_feat\n",
        "\n",
        "        self.init_conv = Conv3(in_channels, n_feat, is_res=True)\n",
        "\n",
        "        self.down1 = UnetDown(n_feat, n_feat)\n",
        "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
        "        self.down3 = UnetDown(2 * n_feat, 2 * n_feat)\n",
        "\n",
        "        self.to_vec = nn.Sequential(nn.AvgPool2d(4), nn.ReLU())\n",
        "\n",
        "        self.timeembed = TimeSiren(2 * n_feat)\n",
        "\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 4, 4),\n",
        "            nn.GroupNorm(8, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.up1 = UnetUp(4 * n_feat, 2 * n_feat)\n",
        "        self.up2 = UnetUp(4 * n_feat, n_feat)\n",
        "        self.up3 = UnetUp(2 * n_feat, n_feat)\n",
        "        self.out = nn.Conv2d(2 * n_feat, self.out_channels, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        down1 = self.down1(x)\n",
        "        down2 = self.down2(down1)\n",
        "        down3 = self.down3(down2)\n",
        "\n",
        "        thro = self.to_vec(down3)\n",
        "        temb = self.timeembed(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "\n",
        "        thro = self.up0(thro + temb)\n",
        "\n",
        "        up1 = self.up1(thro, down3) + temb\n",
        "        up2 = self.up2(up1, down2)\n",
        "        up3 = self.up3(up2, down1)\n",
        "\n",
        "        out = self.out(torch.cat((up3, x), 1))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "goLIIwvk5jOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Denoising Diffusion Probabilistic Model"
      ],
      "metadata": {
        "id": "_qA4jjn7I43w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eps_model: nn.Module,\n",
        "        betas: Tuple[float, float],\n",
        "        n_T: int,\n",
        "        criterion: nn.Module = nn.MSELoss(),\n",
        "    ) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.eps_model = eps_model\n",
        "\n",
        "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.\n",
        "        This implements Algorithm 1 in the paper.\n",
        "        \"\"\"\n",
        "\n",
        "        _ts = torch.randint(1, self.n_T + 1, (x.shape[0],)).to(x.device)\n",
        "        # t ~ Uniform(0, n_T)\n",
        "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "\n",
        "        x_t = (\n",
        "            self.sqrtab[_ts, None, None, None] * x\n",
        "            + self.sqrtmab[_ts, None, None, None] * eps\n",
        "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
        "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
        "\n",
        "        return self.criterion(eps, self.eps_model(x_t, _ts / self.n_T))\n",
        "\n",
        "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
        "\n",
        "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
        "\n",
        "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
        "        for i in range(self.n_T, 0, -1):\n",
        "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
        "            eps = self.eps_model(\n",
        "                x_i, torch.tensor(i / self.n_T).to(device).repeat(n_sample, 1)\n",
        "            )\n",
        "            x_i = (\n",
        "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
        "                + self.sqrt_beta_t[i] * z\n",
        "            )\n",
        "\n",
        "        return x_i\n",
        "\n",
        "\n",
        "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns pre-computed schedules for DDPM sampling, training process.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "\n",
        "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    sqrt_beta_t = torch.sqrt(beta_t)\n",
        "    alpha_t = 1 - beta_t\n",
        "    log_alpha_t = torch.log(alpha_t)\n",
        "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
        "\n",
        "    sqrtab = torch.sqrt(alphabar_t)\n",
        "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
        "\n",
        "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
        "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
        "\n",
        "    return {\n",
        "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
        "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
        "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
        "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
        "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
        "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
        "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
        "    }"
      ],
      "metadata": {
        "id": "QV3QjCFT5rml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DCT Algos"
      ],
      "metadata": {
        "id": "Bl9CGjdGJA1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "    # PyTorch 1.7.0 and newer versions\n",
        "    import torch.fft\n",
        "\n",
        "    def dct1_rfft_impl(x):\n",
        "        return torch.view_as_real(torch.fft.rfft(x, dim=1))\n",
        "    \n",
        "    def dct_fft_impl(v):\n",
        "        return torch.view_as_real(torch.fft.fft(v, dim=1))\n",
        "\n",
        "    def idct_irfft_impl(V):\n",
        "        return torch.fft.irfft(torch.view_as_complex(V), n=V.shape[1], dim=1)\n",
        "except ImportError:\n",
        "    # PyTorch 1.6.0 and older versions\n",
        "    def dct1_rfft_impl(x):\n",
        "        return torch.rfft(x, 1)\n",
        "    \n",
        "    def dct_fft_impl(v):\n",
        "        return torch.rfft(v, 1, onesided=False)\n",
        "\n",
        "    def idct_irfft_impl(V):\n",
        "        return torch.irfft(V, 1, onesided=False)\n",
        "\n",
        "\n",
        "\n",
        "def dct1(x):\n",
        "    \"\"\"\n",
        "    Discrete Cosine Transform, Type I\n",
        "\n",
        "    :param x: the input signal\n",
        "    :return: the DCT-I of the signal over the last dimension\n",
        "    \"\"\"\n",
        "    x_shape = x.shape\n",
        "    x = x.view(-1, x_shape[-1])\n",
        "    x = torch.cat([x, x.flip([1])[:, 1:-1]], dim=1)\n",
        "\n",
        "    return dct1_rfft_impl(x)[:, :, 0].view(*x_shape)\n",
        "\n",
        "\n",
        "def idct1(X):\n",
        "    \"\"\"\n",
        "    The inverse of DCT-I, which is just a scaled DCT-I\n",
        "\n",
        "    Our definition if idct1 is such that idct1(dct1(x)) == x\n",
        "\n",
        "    :param X: the input signal\n",
        "    :return: the inverse DCT-I of the signal over the last dimension\n",
        "    \"\"\"\n",
        "    n = X.shape[-1]\n",
        "    return dct1(X) / (2 * (n - 1))\n",
        "\n",
        "\n",
        "def dct(x, norm=None):\n",
        "    \"\"\"\n",
        "    Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param x: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last dimension\n",
        "    \"\"\"\n",
        "    x_shape = x.shape\n",
        "    N = x_shape[-1]\n",
        "    x = x.contiguous().view(-1, N)\n",
        "\n",
        "    v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)\n",
        "\n",
        "    Vc = dct_fft_impl(v)\n",
        "\n",
        "    k = - torch.arange(N, dtype=x.dtype, device=x.device)[None, :] * np.pi / (2 * N)\n",
        "    W_r = torch.cos(k)\n",
        "    W_i = torch.sin(k)\n",
        "\n",
        "    V = Vc[:, :, 0] * W_r - Vc[:, :, 1] * W_i\n",
        "\n",
        "    if norm == 'ortho':\n",
        "        V[:, 0] /= np.sqrt(N) * 2\n",
        "        V[:, 1:] /= np.sqrt(N / 2) * 2\n",
        "\n",
        "    V = 2 * V.view(*x_shape)\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "def idct(X, norm=None):\n",
        "    \"\"\"\n",
        "    The inverse to DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
        "\n",
        "    Our definition of idct is that idct(dct(x)) == x\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param X: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the inverse DCT-II of the signal over the last dimension\n",
        "    \"\"\"\n",
        "\n",
        "    x_shape = X.shape\n",
        "    N = x_shape[-1]\n",
        "\n",
        "    X_v = X.contiguous().view(-1, x_shape[-1]) / 2\n",
        "\n",
        "    if norm == 'ortho':\n",
        "        X_v[:, 0] *= np.sqrt(N) * 2\n",
        "        X_v[:, 1:] *= np.sqrt(N / 2) * 2\n",
        "\n",
        "    k = torch.arange(x_shape[-1], dtype=X.dtype, device=X.device)[None, :] * np.pi / (2 * N)\n",
        "    W_r = torch.cos(k)\n",
        "    W_i = torch.sin(k)\n",
        "\n",
        "    V_t_r = X_v\n",
        "    V_t_i = torch.cat([X_v[:, :1] * 0, -X_v.flip([1])[:, :-1]], dim=1)\n",
        "\n",
        "    V_r = V_t_r * W_r - V_t_i * W_i\n",
        "    V_i = V_t_r * W_i + V_t_i * W_r\n",
        "\n",
        "    V = torch.cat([V_r.unsqueeze(2), V_i.unsqueeze(2)], dim=2)\n",
        "\n",
        "    v = idct_irfft_impl(V)\n",
        "    x = v.new_zeros(v.shape)\n",
        "    x[:, ::2] += v[:, :N - (N // 2)]\n",
        "    x[:, 1::2] += v.flip([1])[:, :N // 2]\n",
        "\n",
        "    return x.view(*x_shape)\n",
        "\n",
        "\n",
        "def dct_2d(x, norm=None):\n",
        "    \"\"\"\n",
        "    2-dimentional Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param x: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last 2 dimensions\n",
        "    \"\"\"\n",
        "    X1 = dct(x, norm=norm)\n",
        "    X2 = dct(X1.transpose(-1, -2), norm=norm)\n",
        "    return X2.transpose(-1, -2)\n",
        "\n",
        "\n",
        "def idct_2d(X, norm=None):\n",
        "    \"\"\"\n",
        "    The inverse to 2D DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
        "\n",
        "    Our definition of idct is that idct_2d(dct_2d(x)) == x\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param X: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last 2 dimensions\n",
        "    \"\"\"\n",
        "    x1 = idct(X, norm=norm)\n",
        "    x2 = idct(x1.transpose(-1, -2), norm=norm)\n",
        "    return x2.transpose(-1, -2)\n",
        "\n",
        "\n",
        "def dct_3d(x, norm=None):\n",
        "    \"\"\"\n",
        "    3-dimentional Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param x: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last 3 dimensions\n",
        "    \"\"\"\n",
        "    X1 = dct(x, norm=norm)\n",
        "    X2 = dct(X1.transpose(-1, -2), norm=norm)\n",
        "    X3 = dct(X2.transpose(-1, -3), norm=norm)\n",
        "    return X3.transpose(-1, -3).transpose(-1, -2)\n",
        "\n",
        "\n",
        "def idct_3d(X, norm=None):\n",
        "    \"\"\"\n",
        "    The inverse to 3D DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
        "\n",
        "    Our definition of idct is that idct_3d(dct_3d(x)) == x\n",
        "\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "\n",
        "    :param X: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last 3 dimensions\n",
        "    \"\"\"\n",
        "    x1 = idct(X, norm=norm)\n",
        "    x2 = idct(x1.transpose(-1, -2), norm=norm)\n",
        "    x3 = idct(x2.transpose(-1, -3), norm=norm)\n",
        "    return x3.transpose(-1, -3).transpose(-1, -2)\n",
        "\n",
        "\n",
        "class LinearDCT(nn.Linear):\n",
        "    \"\"\"Implement any DCT as a linear layer; in practice this executes around\n",
        "    50x faster on GPU. Unfortunately, the DCT matrix is stored, which will \n",
        "    increase memory usage.\n",
        "    :param in_features: size of expected input\n",
        "    :param type: which dct function in this file to use\"\"\"\n",
        "    def __init__(self, in_features, type, norm=None, bias=False):\n",
        "        self.type = type\n",
        "        self.N = in_features\n",
        "        self.norm = norm\n",
        "        super(LinearDCT, self).__init__(in_features, in_features, bias=bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # initialise using dct function\n",
        "        I = torch.eye(self.N)\n",
        "        if self.type == 'dct1':\n",
        "            self.weight.data = dct1(I).data.t()\n",
        "        elif self.type == 'idct1':\n",
        "            self.weight.data = idct1(I).data.t()\n",
        "        elif self.type == 'dct':\n",
        "            self.weight.data = dct(I, norm=self.norm).data.t()\n",
        "        elif self.type == 'idct':\n",
        "            self.weight.data = idct(I, norm=self.norm).data.t()\n",
        "        self.weight.requires_grad = False # don't learn this!\n",
        "\n",
        "\n",
        "def apply_linear_2d(x, linear_layer):\n",
        "    \"\"\"Can be used with a LinearDCT layer to do a 2D DCT.\n",
        "    :param x: the input signal\n",
        "    :param linear_layer: any PyTorch Linear layer\n",
        "    :return: result of linear layer applied to last 2 dimensions\n",
        "    \"\"\"\n",
        "    X1 = linear_layer(x)\n",
        "    X2 = linear_layer(X1.transpose(-1, -2))\n",
        "    return X2.transpose(-1, -2)\n",
        "\n",
        "def apply_linear_3d(x, linear_layer):\n",
        "    \"\"\"Can be used with a LinearDCT layer to do a 3D DCT.\n",
        "    :param x: the input signal\n",
        "    :param linear_layer: any PyTorch Linear layer\n",
        "    :return: result of linear layer applied to last 3 dimensions\n",
        "    \"\"\"\n",
        "    X1 = linear_layer(x)\n",
        "    X2 = linear_layer(X1.transpose(-1, -2))\n",
        "    X3 = linear_layer(X2.transpose(-1, -3))\n",
        "    return X3.transpose(-1, -3).transpose(-1, -2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.Tensor(1000,4096)\n",
        "    x.normal_(0,1)\n",
        "    linear_dct = LinearDCT(4096, 'dct')\n",
        "    error = torch.abs(dct(x) - linear_dct(x))\n",
        "    assert error.max() < 1e-3, (error, error.max())\n",
        "    linear_idct = LinearDCT(4096, 'idct')\n",
        "    error = torch.abs(idct(x) - linear_idct(x))\n",
        "    assert error.max() < 1e-3, (error, error.max())\n"
      ],
      "metadata": {
        "id": "3mCHSU6wIdn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "b8digpV9NnP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Torch Dataloader Transforms"
      ],
      "metadata": {
        "id": "XJX8bgTUJPns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "class ConvertImageToDCT(torch.nn.Module):\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "      print(\"Init ConvertImageToDCT\")\n",
        "\n",
        "  def __call__(self, pic):\n",
        "    res = dct1(pic)\n",
        "    return res\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "      return f\"{self.__class__.__name__}()\"\n",
        "\n",
        "class ConvertDCTToImage(torch.nn.Module):\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "      print(\"Init ConvertDCTToImage\")\n",
        "\n",
        "  def __call__(self, dct):\n",
        "    # print(type(pic))\n",
        "    img = idct1(dct)\n",
        "    # print(type(res))\n",
        "    return img\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "      return f\"{self.__class__.__name__}()\""
      ],
      "metadata": {
        "id": "421MpMBOHNli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train DDPM on CIFAR"
      ],
      "metadata": {
        "id": "eZLjKBSjJk1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Optional, Tuple\n",
        "from sympy import Ci\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "# from mindiffusion.unet import NaiveUnet\n",
        "# from mindiffusion.ddpm import DDPM\n",
        "\n",
        "\n",
        "def train_cifar10(\n",
        "    n_epoch: int = 1, device: str = \"cuda:0\", load_pth: Optional[str] = None, use_dct_transform = False\n",
        ") -> None:\n",
        "\n",
        "    ddpm = DDPM(eps_model=NaiveUnet(3, 3, n_feat=128), betas=(1e-4, 0.02), n_T=1000)\n",
        "\n",
        "    if load_pth is not None:\n",
        "        ddpm.load_state_dict(torch.load(\"ddpm_cifar.pth\"))\n",
        "\n",
        "    ddpm.to(device)\n",
        "    tf = None\n",
        "    if use_dct_transform:\n",
        "      tf = transforms.Compose(\n",
        "        [\n",
        "          transforms.ToTensor(), \n",
        "          ConvertImageToDCT(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ]\n",
        "      )\n",
        "    else:\n",
        "      tf = transforms.Compose(\n",
        "        [\n",
        "          transforms.ToTensor(), \n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ]\n",
        "      )\n",
        "\n",
        "    dataset = CIFAR10(\n",
        "        \"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=tf,\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=2)\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=1e-5)\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        print(f\"Epoch {i} : \")\n",
        "        ddpm.train()\n",
        "\n",
        "        pbar = tqdm(dataloader)\n",
        "        loss_ema = None\n",
        "        for x, _ in pbar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_ema is None:\n",
        "                loss_ema = loss.item()\n",
        "            else:\n",
        "                loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
        "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(8, (3, 32, 32), device)\n",
        "\n",
        "            if use_dct_transform:\n",
        "              ixx = []\n",
        "              iyy = []\n",
        "              for xx in xh.cpu().detach():\n",
        "                ixx.append((idct1(xx)))\n",
        "              for yy in x[:8].cpu().detach():\n",
        "                iyy.append((idct1(yy)))\n",
        "              ixx = torch.stack(ixx)\n",
        "              iyy = torch.stack(iyy)\n",
        "        \n",
        "              xset = torch.cat([ixx, iyy], dim=0)\n",
        "              grid = make_grid(xset, normalize=True, value_range=(-1, 1), nrow=4)\n",
        "              save_image(grid, f\"/content/images/dct_ddpm_sample_cifar{i}.png\")\n",
        "              # save model\n",
        "              torch.save(ddpm.state_dict(), f\"./dct_ddpm_cifar.pth\")\n",
        "            else:\n",
        "              xset = torch.cat([xh, x[:8]], dim=0)\n",
        "              grid = make_grid(xset, normalize=True, value_range=(-1, 1), nrow=4)\n",
        "              save_image(grid, f\"/content/images/ddpm_sample_cifar{i}.png\")\n",
        "              # save model\n",
        "              torch.save(ddpm.state_dict(), f\"./ddpm_cifar.pth\")"
      ],
      "metadata": {
        "id": "0u4Whgf2uPFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Begin Training"
      ],
      "metadata": {
        "id": "GrtiT0DnUdq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(n_epoch=50, use_dct_transform=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-CVHSFNUxD7",
        "outputId": "6a9e7426-6b49-4752-bdeb-132e90ac92c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.3452:  72%|███████▏  | 71/98 [00:53<00:20,  1.34it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cifar10(n_epoch=50, use_dct_transform=False)"
      ],
      "metadata": {
        "id": "queVPwNqDNyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "UUDImX_mUTxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FnbeK8I5P1X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\"\n",
        "ddpm = DDPM(eps_model=NaiveUnet(3, 3, n_feat=8), betas=(1e-4, 0.02), n_T=10)\n",
        "ddpm.to(device)\n",
        "xh = ddpm.sample(8, (3, 32, 32), device)"
      ],
      "metadata": {
        "id": "f-AExh93L8gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xh.shape"
      ],
      "metadata": {
        "id": "sZtV0IMUREAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xh.unsqueeze(dim=0).shape"
      ],
      "metadata": {
        "id": "VJ5MI3rGRsgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zz = []\n",
        "for i in xh.cpu().detach():\n",
        "  print(i.shape)\n",
        "  print(type(i))\n",
        "  print(\"Plotting\")\n",
        "  print(torch.Tensor(idct1(i)).shape)\n",
        "  if zz == None:\n",
        "    zz = torch.Tensor(idct1(i))\n",
        "  else:\n",
        "    zz.append(torch.Tensor(idct1(i)))\n",
        "  plt.imshow((idct1(i)).T)\n",
        "  plt.show()\n",
        "\n",
        "zz = torch.stack(zz)"
      ],
      "metadata": {
        "id": "TosgQhnHR5M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zz.shape"
      ],
      "metadata": {
        "id": "M498P2I0VG3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gQfMj6ssW_RX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}